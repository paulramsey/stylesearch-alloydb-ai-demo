{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "XJ1fNT-8xEQx",
      "metadata": {
        "id": "XJ1fNT-8xEQx"
      },
      "source": [
        "# Cymbal Shops StyleSearch Demo - Multimodal Hybrid Product Search on AlloyDB\n",
        "\n",
        "This notebook deploys the backend for a demo of Hybrid Search in [AlloyDB for PostgreSQL](https://cloud.google.com/products/alloydb?e=48754805&hl=en). It combines traditional SQL with text embeddings ([`text-embedding-005`](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings)), multimodal vector embeddings ([`multimodalembedding@001`](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings)), and full-text search ([Generalized Inverted Index](https://www.postgresql.org/docs/current/gin.html)) with [Reciprocal Rank Fusion](https://medium.com/@devalshah1619/mathematical-intuition-behind-reciprocal-rank-fusion-rrf-explained-in-2-mins-002df0cc5e2a) re-ranking for enhanced product search.\n",
        "\n",
        "> NOTE: This notebook uses pre-prepared data to quickly spin up a demo environment. You can see the steps that were required to prepare the data in the accompanying [data prep notebook](./cymbal_shops_hybrid_search_alloydb_data_prep.ipynb).\n",
        "\n",
        "> **IMPORTANT:** This notebook leverages Preview features in AlloyDB AI. **[Sign up for the preview](https://docs.google.com/forms/d/e/1FAIpQLSfJ9vHIJ79nI7JWBDELPFL75pDQa4XVZQ2fxShfYddW0RwmLw/viewform)** before running this notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R6yilxF48QrV",
      "metadata": {
        "id": "R6yilxF48QrV"
      },
      "source": [
        "## Basic Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AJ1QMVgDWkvP",
      "metadata": {
        "id": "AJ1QMVgDWkvP"
      },
      "source": [
        "### Define Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zrQSpdwg1rwJweglQjYzUhDo",
      "metadata": {
        "id": "zrQSpdwg1rwJweglQjYzUhDo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Update these variables to match your environment\n",
        "project_id = \"your-project\"  # @param {type:\"string\"}\n",
        "region = \"your-region\"  # @param {type:\"string\"}\n",
        "alloydb_cluster = \"your-alloydb-cluster\"  # @param {type:\"string\"}\n",
        "alloydb_instance = \"your-alloydb-instance\"  # @param {type:\"string\"}\n",
        "alloydb_password = input(\"Please provide a password to be used for 'postgres' database user: \")\n",
        "\n",
        "# Don't change values below this line.\n",
        "alloydb_database = \"ecom\" \n",
        "database_backup_uri = \"gs://pr-public-demo-data/alloydb-retail-demo/alloydb-export/ecom.sql\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IX5fm685HuB3",
      "metadata": {
        "id": "IX5fm685HuB3"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kv16h5fhHt7m",
      "metadata": {
        "id": "kv16h5fhHt7m"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet google-cloud-storage \\\n",
        "                      google-cloud-aiplatform \\\n",
        "                      asyncpg \\\n",
        "                      google.cloud.alloydb.connector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DeUbHclxw7_l",
      "metadata": {
        "id": "DeUbHclxw7_l"
      },
      "source": [
        "### Authenticate to Google Cloud within Colab\n",
        "In order to access your Google Cloud Project from this notebook, you will need to Authenticate as an IAM user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_Q9hyqdyEx6l",
      "metadata": {
        "id": "_Q9hyqdyEx6l"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UCiNGP1Qxd6x",
      "metadata": {
        "id": "UCiNGP1Qxd6x"
      },
      "source": [
        "### Connect Your Google Cloud Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SLUGlG6UE2CK",
      "metadata": {
        "id": "SLUGlG6UE2CK"
      },
      "outputs": [],
      "source": [
        "# Configure gcloud.\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O-oqMC5Ox-ZM",
      "metadata": {
        "id": "O-oqMC5Ox-ZM"
      },
      "source": [
        "### Enable APIs for AlloyDB and Vertex AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CKWrwyfzyTwH",
      "metadata": {
        "id": "CKWrwyfzyTwH"
      },
      "outputs": [],
      "source": [
        "!gcloud services enable alloydb.googleapis.com aiplatform.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oIk5GxbnFaE3",
      "metadata": {
        "id": "oIk5GxbnFaE3"
      },
      "source": [
        "### Configure Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wvYGGRRoFXl4",
      "metadata": {
        "id": "wvYGGRRoFXl4"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "# Configure the root logger to output messages with INFO level or above\n",
        "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='%(asctime)s[%(levelname)5s][%(name)14s] - %(message)s',  datefmt='%H:%M:%S', force=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GoH1gbi_jEiJ",
      "metadata": {
        "id": "GoH1gbi_jEiJ"
      },
      "source": [
        "### Define Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cg0RdzTkIhlh",
      "metadata": {
        "id": "cg0RdzTkIhlh"
      },
      "source": [
        "#### rest_api_helper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "st4dRKZklRc4",
      "metadata": {
        "id": "st4dRKZklRc4"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import google.auth\n",
        "import json\n",
        "\n",
        "# Get an access token based upon the current user\n",
        "creds, _ = google.auth.default()\n",
        "authed_session = google.auth.transport.requests.AuthorizedSession(creds)\n",
        "access_token=creds.token\n",
        "\n",
        "if project_id:\n",
        "  authed_session.headers.update({\"x-goog-user-project\": project_id}) # Required to workaround a project quota bug\n",
        "\n",
        "def rest_api_helper(\n",
        "    session: requests.Session,\n",
        "    url: str,\n",
        "    http_verb: str,\n",
        "    request_body: dict = None,\n",
        "    params: dict = None\n",
        "  ) -> dict:\n",
        "  \"\"\"Calls a REST API using a pre-authenticated requests Session.\"\"\"\n",
        "\n",
        "  headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "  try:\n",
        "\n",
        "    if http_verb == \"GET\":\n",
        "      response = session.get(url, headers=headers, params=params)\n",
        "    elif http_verb == \"POST\":\n",
        "      response = session.post(url, json=request_body, headers=headers, params=params)\n",
        "    elif http_verb == \"PUT\":\n",
        "      response = session.put(url, json=request_body, headers=headers, params=params)\n",
        "    elif http_verb == \"PATCH\":\n",
        "      response = session.patch(url, json=request_body, headers=headers, params=params)\n",
        "    elif http_verb == \"DELETE\":\n",
        "      response = session.delete(url, headers=headers, params=params)\n",
        "    else:\n",
        "      raise ValueError(f\"Unknown HTTP verb: {http_verb}\")\n",
        "\n",
        "    # Raise an exception for bad status codes (4xx or 5xx)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Check if response has content before trying to parse JSON\n",
        "    if response.content:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return {} # Return empty dict for empty responses (like 204 No Content)\n",
        "\n",
        "  except requests.exceptions.RequestException as e:\n",
        "      # Catch potential requests library errors (network, timeout, etc.)\n",
        "      # Log detailed error information\n",
        "      print(f\"Request failed: {e}\")\n",
        "      if e.response is not None:\n",
        "          print(f\"Request URL: {e.request.url}\")\n",
        "          print(f\"Request Headers: {e.request.headers}\")\n",
        "          print(f\"Request Body: {e.request.body}\")\n",
        "          print(f\"Response Status: {e.response.status_code}\")\n",
        "          print(f\"Response Text: {e.response.text}\")\n",
        "          # Re-raise a more specific error or a custom one\n",
        "          raise RuntimeError(f\"API call failed with status {e.response.status_code}: {e.response.text}\") from e\n",
        "      else:\n",
        "          raise RuntimeError(f\"API call failed: {e}\") from e\n",
        "  except json.JSONDecodeError as e:\n",
        "      print(f\"Failed to decode JSON response: {e}\")\n",
        "      print(f\"Response Text: {response.text}\")\n",
        "      raise RuntimeError(f\"Invalid JSON received from API: {response.text}\") from e\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ol_OBUX4Lr4m",
      "metadata": {
        "id": "Ol_OBUX4Lr4m"
      },
      "source": [
        "#### run_query()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rLn6HEQb85DQ",
      "metadata": {
        "id": "rLn6HEQb85DQ"
      },
      "outputs": [],
      "source": [
        "# Create AlloyDB Query Helper Function\n",
        "import sqlalchemy\n",
        "from sqlalchemy import text, exc\n",
        "import pandas as pd\n",
        "\n",
        "async def run_query(pool, sql: str, params = None, output_as_df: bool = True):\n",
        "    \"\"\"Executes a SQL query or statement against the database pool.\n",
        "\n",
        "    Handles various SQL statements:\n",
        "    - SELECT/WITH: Returns results as a DataFrame (if output_as_df=True)\n",
        "      or ResultProxy. Supports parameters. Does not commit.\n",
        "    - EXPLAIN/EXPLAIN ANALYZE: Executes the explain, returns the query plan\n",
        "      as a formatted multi-line string. Ignores output_as_df.\n",
        "      Supports parameters. Does not commit.\n",
        "    - INSERT/UPDATE/DELETE/CREATE/ALTER etc.: Executes the statement,\n",
        "      commits the transaction, logs info, and returns the ResultProxy.\n",
        "      Supports single or bulk parameters (executemany).\n",
        "\n",
        "    Args:\n",
        "      pool: An asynchronous SQLAlchemy connection pool.\n",
        "      sql: A string containing the SQL query or statement template.\n",
        "      params: Optional.\n",
        "        - None: Execute raw SQL (Use with caution for non-SELECT/EXPLAIN).\n",
        "        - dict or tuple: Parameters for a single execution.\n",
        "        - list of dicts/tuples: Parameters for bulk execution (executemany).\n",
        "      output_as_df (bool): If True and query is SELECT/WITH, return pandas DataFrame.\n",
        "                           Ignored for EXPLAIN and non-data-returning statements.\n",
        "\n",
        "    Returns:\n",
        "      pandas.DataFrame | str | sqlalchemy.engine.Result | None:\n",
        "        - DataFrame: For SELECT/WITH if output_as_df=True.\n",
        "        - str: For EXPLAIN/EXPLAIN ANALYZE, containing the formatted query plan.\n",
        "        - ResultProxy: For non-SELECT/WITH/EXPLAIN statements, or SELECT/WITH\n",
        "                       if output_as_df=False.\n",
        "        - None: If a SQLAlchemy ProgrammingError or other specific error occurs.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Catches and logs `sqlalchemy.exc.ProgrammingError`, returning None.\n",
        "                   May re-raise other database exceptions.\n",
        "\n",
        "    Example Execution:\n",
        "      Single SELECT:\n",
        "        sql_select = \"SELECT ticker, company_name from investments LIMIT 5\"\n",
        "        df_result = await run_query(pool, sql_select)\n",
        "\n",
        "      Single non-SELECT - Parameterized (Safe!):\n",
        "        Parameterized INSERT:\n",
        "          sql_insert = \"INSERT INTO investments (ticker, company_name) VALUES (:ticker, :name)\"\n",
        "          params_insert = {\"ticker\": \"NEW\", \"name\": \"New Company\"}\n",
        "          insert_result = await run_query(pool, sql_insert, params_insert)\n",
        "\n",
        "        Parameterized UPDATE:\n",
        "          sql_update = \"UPDATE products SET price = :price WHERE id = :product_id\"\n",
        "          params_update = {\"price\": 99.99, \"product_id\": 123}\n",
        "          update_result = await run_query(pool, sql_update, params_update)\n",
        "\n",
        "      Bulk Update:\n",
        "        docs = pd.DataFrame([\n",
        "            {'id': 101, 'sparse_embedding': '[0.1, 0.2]'},\n",
        "            {'id': 102, 'sparse_embedding': '[0.3, 0.4]'},\n",
        "            # ... more rows\n",
        "        ])\n",
        "\n",
        "        update_sql_template = '''\n",
        "            UPDATE products\n",
        "            SET sparse_embedding = :embedding,\n",
        "                sparse_embedding_model = 'BM25'\n",
        "            WHERE id = :product_id\n",
        "        ''' # Using named parameters :param_name\n",
        "\n",
        "        # Prepare list of dictionaries for params\n",
        "        data_to_update = [\n",
        "            {\"embedding\": row.sparse_embedding, \"product_id\": row.id}\n",
        "            for row in docs.itertuples(index=False)\n",
        "        ]\n",
        "\n",
        "        if data_to_update:\n",
        "          bulk_result = await run_query(pool, update_sql_template, data_to_update)\n",
        "          # bulk_result is the SQLAlchemy ResultProxy\n",
        "\n",
        "    \"\"\"\n",
        "    sql_lower_stripped = sql.strip().lower()\n",
        "    is_select_with = sql_lower_stripped.startswith(('select', 'with'))\n",
        "    is_explain = sql_lower_stripped.startswith('explain')\n",
        "\n",
        "    # Determine if the statement is expected to return data rows or a plan\n",
        "    is_data_returning = is_select_with or is_explain\n",
        "\n",
        "    # Determine actual DataFrame output eligibility (only for SELECT/WITH)\n",
        "    effective_output_as_df = output_as_df and is_select_with\n",
        "\n",
        "    # Check if params suggest a bulk operation (for logging purposes)\n",
        "    is_bulk_operation = isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], (dict, tuple, list))\n",
        "\n",
        "    async with pool.connect() as conn:\n",
        "        try:\n",
        "          # Execute with or without params\n",
        "          if params:\n",
        "              result = await conn.execute(text(sql), params)\n",
        "          else:\n",
        "              # Add warning for raw SQL only if it's NOT data-returning\n",
        "              #if not is_data_returning:\n",
        "                  #logging.warning(\"Executing non-SELECT/EXPLAIN raw SQL without parameters. Ensure SQL is safe.\")\n",
        "              result = await conn.execute(text(sql))\n",
        "\n",
        "          # --- Handle statements that return data or plan ---\n",
        "          if is_data_returning:\n",
        "              if is_explain:\n",
        "                  # Fetch and format EXPLAIN output as a string\n",
        "                    try:\n",
        "                        plan_rows = result.fetchall()\n",
        "                        # EXPLAIN output is usually text in the first column\n",
        "                        query_plan = \"\\n\".join([str(row[0]) for row in plan_rows])\n",
        "                        #logging.info(f\"EXPLAIN executed successfully for: {sql[:100]}...\")\n",
        "                        return query_plan\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error fetching/formatting EXPLAIN result: {e}\")\n",
        "                        return None\n",
        "              else: # Handle SELECT / WITH\n",
        "                  if effective_output_as_df:\n",
        "                      try:\n",
        "                          rows = result.fetchall()\n",
        "                          column_names = result.keys()\n",
        "                          df = pd.DataFrame(rows, columns=column_names)\n",
        "                          #logging.info(f\"SELECT/WITH executed successfully, returning DataFrame for: {sql[:100]}...\")\n",
        "                          return df\n",
        "                      except Exception as e:\n",
        "                          logging.error(f\"Error converting SELECT result to DataFrame: {e}\")\n",
        "                          logging.info(f\"Returning raw ResultProxy for SELECT/WITH due to DataFrame conversion error for: {sql[:100]}...\")\n",
        "                          return result # Fallback to raw result\n",
        "                  else:\n",
        "                      # Return raw result proxy for SELECT/WITH if df output not requested\n",
        "                      #logging.info(f\"SELECT/WITH executed successfully, returning ResultProxy for: {sql[:100]}...\")\n",
        "                      return result\n",
        "\n",
        "          # --- Handle Non-Data Returning Statements (INSERT, UPDATE, DELETE, CREATE, etc.) ---\n",
        "          else:\n",
        "              await conn.commit() # Commit changes ONLY for these statements\n",
        "              operation_type = sql.strip().split()[0].upper()\n",
        "              row_count = result.rowcount # Note: rowcount behavior varies\n",
        "\n",
        "              if is_bulk_operation:\n",
        "                  print(f\"Bulk {operation_type} executed for {len(params)} items. Result rowcount: {row_count}\")\n",
        "              elif operation_type in ['INSERT', 'UPDATE', 'DELETE']:\n",
        "                  print(f\"{operation_type} statement executed successfully. {row_count} row(s) affected.\")\n",
        "              else: # CREATE, ALTER, etc.\n",
        "                  print(f\"{operation_type} statement executed successfully. Result rowcount: {row_count}\")\n",
        "              return result # Return the result proxy\n",
        "\n",
        "        except exc.ProgrammingError as e:\n",
        "            # Log the error with context\n",
        "            logging.error(f\"SQL Programming Error executing query:\\nSQL: {sql[:500]}...\\nParams (sample): {str(params)[:500]}...\\nError: {e}\")\n",
        "            # Rollback might happen automatically on context exit with error, but explicit can be clearer\n",
        "            # await conn.rollback() # Consider if needed based on pool/transaction settings\n",
        "            return None # Return None on handled programming errors\n",
        "        except Exception as e:\n",
        "            # Log other unexpected errors\n",
        "            logging.error(f\"An unexpected error occurred during query execution:\\nSQL: {sql[:500]}...\\nError: {e}\")\n",
        "            # await conn.rollback() # Consider if needed\n",
        "            raise # Re-raise unexpected errors\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TduOO0v1Z8Zw",
      "metadata": {
        "id": "TduOO0v1Z8Zw"
      },
      "source": [
        "## Create an AlloyDB Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gn8g7-wCyZU6",
      "metadata": {
        "id": "Gn8g7-wCyZU6"
      },
      "source": [
        "You will need an AlloyDB for PostgreSQL cluster to use this notebook. If you already have an AlloyDB cluster, you can skip to the `Setup Database` section. Otherwise, use the cells below to create one.\n",
        "\n",
        "> â³ - Creating an AlloyDB cluster may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MQYni0NlTLzC",
      "metadata": {
        "id": "MQYni0NlTLzC"
      },
      "outputs": [],
      "source": [
        "# create the AlloyDB Cluster\n",
        "!gcloud beta alloydb clusters create {alloydb_cluster} --password={alloydb_password} --region={region}\n",
        "\n",
        "# Create the AlloyDB Instance\n",
        "!gcloud beta alloydb instances create {alloydb_instance} --instance-type=PRIMARY --cpu-count=2 --region={region} --cluster={alloydb_cluster}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BXsQ1UJv4ZVJ",
      "metadata": {
        "id": "BXsQ1UJv4ZVJ"
      },
      "source": [
        "To connect to your AlloyDB instance from this notebook, you will need to enable public IP on your instance. Alternatively, you can follow [these instructions](https://cloud.google.com/alloydb/docs/connect-external) to connect to an AlloyDB for PostgreSQL instance with Private IP from outside your VPC. You can also use the `--authorized-external-networks` flag to limit communication over public IP to specific IP address ranges if desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OPVWsQB04Yyl",
      "metadata": {
        "id": "OPVWsQB04Yyl"
      },
      "outputs": [],
      "source": [
        "# Enable Public IP on AlloyDB\n",
        "!gcloud beta alloydb instances update {alloydb_instance} --region={region} --cluster={alloydb_cluster} --assign-inbound-public-ip=ASSIGN_IPV4 --database-flags=\"password.enforce_complexity=on\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AtNy6c5Y_X5b",
      "metadata": {
        "id": "AtNy6c5Y_X5b"
      },
      "source": [
        "## Setup Database"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OtsWx7uQ8Rxj",
      "metadata": {
        "id": "OtsWx7uQ8Rxj"
      },
      "source": [
        "### Connect to the AlloyDB Cluster\n",
        "\n",
        "This function will create a connection pool to your AlloyDB instance using the AlloyDB Python connector. The AlloyDB Python connector will automatically create secure connections to your AlloyDB instance using mTLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q0GMkdwD8Ldr",
      "metadata": {
        "id": "Q0GMkdwD8Ldr"
      },
      "outputs": [],
      "source": [
        "import asyncpg\n",
        "\n",
        "import sqlalchemy\n",
        "from sqlalchemy.ext.asyncio import AsyncEngine, create_async_engine\n",
        "\n",
        "from google.cloud.alloydb.connector import AsyncConnector, IPTypes\n",
        "\n",
        "async def init_connection_pool(connector: AsyncConnector, db_name: str = alloydb_database, pool_size: int = 5) -> AsyncEngine:\n",
        "    # initialize Connector object for connections to AlloyDB\n",
        "    connection_string = f\"projects/{project_id}/locations/{region}/clusters/{alloydb_cluster}/instances/{alloydb_instance}\"\n",
        "\n",
        "    async def getconn() -> asyncpg.Connection:\n",
        "        conn: asyncpg.Connection = await connector.connect(\n",
        "            connection_string,\n",
        "            \"asyncpg\",\n",
        "            user=\"postgres\",\n",
        "            password=alloydb_password,\n",
        "            db=db_name,\n",
        "            ip_type=IPTypes.PUBLIC, # # Optionally use IPTypes.PRIVATE\n",
        "        )\n",
        "        return conn\n",
        "\n",
        "    pool = create_async_engine(\n",
        "        \"postgresql+asyncpg://\",\n",
        "        async_creator=getconn,\n",
        "        pool_size=pool_size,\n",
        "        max_overflow=0,\n",
        "        isolation_level='AUTOCOMMIT'\n",
        "    )\n",
        "    return pool\n",
        "\n",
        "connector = AsyncConnector()\n",
        "\n",
        "postgres_db_pool = await init_connection_pool(connector, \"postgres\")\n",
        "ecom_db_pool = await init_connection_pool(connector, f\"{alloydb_database}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3vrrT_n4MQ9I",
      "metadata": {
        "id": "3vrrT_n4MQ9I"
      },
      "source": [
        "### Import Sample Data to AlloyDB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WzevAboCf1xx",
      "metadata": {
        "id": "WzevAboCf1xx"
      },
      "source": [
        "### Add Required Permissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OKu9MWhzid30",
      "metadata": {
        "id": "OKu9MWhzid30"
      },
      "outputs": [],
      "source": [
        "project_number = ! gcloud projects describe {project_id} --format='value(projectNumber)'\n",
        "project_number = project_number[0]\n",
        "\n",
        "# These permissions are required to read from GCS for the data import and to integrate with Vertex AI for on-the-fly embedding generation.\n",
        "roles_array = [\n",
        "    \"roles/storage.admin\",\n",
        "    \"roles/aiplatform.user\"\n",
        "]\n",
        "\n",
        "for r in roles_array:\n",
        "  ! gcloud projects add-iam-policy-binding {project_id} \\\n",
        "      --member=\"serviceAccount:service-{project_number}@gcp-sa-alloydb.iam.gserviceaccount.com\" \\\n",
        "      --role=\"{r}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TpeQdjJZ577L",
      "metadata": {
        "id": "TpeQdjJZ577L"
      },
      "source": [
        "### Create Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JGA7etBk568a",
      "metadata": {
        "id": "JGA7etBk568a"
      },
      "outputs": [],
      "source": [
        "# Create the database\n",
        "sql = f\"CREATE DATABASE {alloydb_database};\"\n",
        "await run_query(postgres_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50cj8jPA7F2z",
      "metadata": {
        "id": "50cj8jPA7F2z"
      },
      "source": [
        "### Install Pre-requisite Extensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J3y6BXLf7FwR",
      "metadata": {
        "id": "J3y6BXLf7FwR"
      },
      "outputs": [],
      "source": [
        "sql_array = []\n",
        "\n",
        "sql_array.append(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
        "sql_array.append(\"CREATE EXTENSION IF NOT EXISTS google_ml_integration;\")\n",
        "sql_array.append(\"CREATE EXTENSION IF NOT EXISTS alloydb_scann;\")\n",
        "\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ba90177",
      "metadata": {},
      "source": [
        "### Create Agentspace User\n",
        "\n",
        "This step creates a user for an options Agentspace integration. It is required for a successful import in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89bcf9b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "agentspace_user_password = input(\"Please provide a password to be used for 'agentspace_user' database user: \")\n",
        "sql = f\"CREATE ROLE agentspace_user WITH LOGIN PASSWORD '{agentspace_user_password}';\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ufkdb1Iy3JJM",
      "metadata": {
        "id": "Ufkdb1Iy3JJM"
      },
      "source": [
        "### Run the Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56F76nif1pY",
      "metadata": {
        "id": "f56F76nif1pY"
      },
      "outputs": [],
      "source": [
        "# Reference: https://cloud.google.com/alloydb/docs/reference/rest/v1/projects.locations.clusters/import\n",
        "#            https://cloud.google.com/alloydb/docs/import-sql-file\n",
        "\n",
        "import time\n",
        "\n",
        "url = f\"https://alloydb.googleapis.com/v1/projects/{project_id}/locations/{region}/clusters/{alloydb_cluster}:import\"\n",
        "request_body = {\n",
        "   \"gcsUri\": f\"{database_backup_uri}\",\n",
        "   \"database\": f\"{alloydb_database}\",\n",
        "   \"user\": \"postgres\",\n",
        "   \"sqlImportOptions\": {}\n",
        "}\n",
        "\n",
        "result = rest_api_helper(authed_session, url, 'POST', request_body, {})\n",
        "print(f\"Kicked off import: {result}\")\n",
        "\n",
        "operation_id = result['name']\n",
        "\n",
        "operation_complete = False\n",
        "while operation_complete == False:\n",
        "  print(f\"Import still running: {operation_id}\")\n",
        "  url = f\"https://alloydb.googleapis.com/v1/{operation_id}\"\n",
        "  response = rest_api_helper(authed_session, url, 'GET', request_body, {})\n",
        "  operation_complete = response['done']\n",
        "  if operation_complete:\n",
        "    print(f\"Operation complete. Check result payload for potential errors. \\nResult: {response}\")\n",
        "    continue\n",
        "  time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qsc8Zsf2srYu",
      "metadata": {
        "id": "qsc8Zsf2srYu"
      },
      "source": [
        "### Check Row Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XyiXMrtzbrFz",
      "metadata": {
        "id": "XyiXMrtzbrFz"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"\n",
        "SELECT 'distribution_centers' AS table_name, (SELECT COUNT(*) FROM distribution_centers) AS actual_row_count, 10 AS target_row_count\n",
        "UNION ALL\n",
        "SELECT 'events', (SELECT COUNT(*) FROM events), 2438862\n",
        "UNION ALL\n",
        "SELECT 'inventory_items', (SELECT COUNT(*) FROM inventory_items), 494254\n",
        "UNION ALL\n",
        "SELECT 'orders', (SELECT COUNT(*) FROM orders), 125905\n",
        "UNION ALL\n",
        "SELECT 'order_items', (SELECT COUNT(*) FROM order_items), 182905\n",
        "UNION ALL\n",
        "SELECT 'products', (SELECT COUNT(*) FROM products), 29120\n",
        "UNION ALL\n",
        "SELECT 'users', (SELECT COUNT(*) FROM users), 100000;\n",
        "\"\"\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "as-XHQMY-YuP",
      "metadata": {
        "id": "as-XHQMY-YuP"
      },
      "source": [
        "### Test the Vertex AI Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RSZn7lfc5R3O",
      "metadata": {
        "id": "RSZn7lfc5R3O"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT embedding('text-embedding-005', 'This string will be transformed into an embedding.');\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58186a5c",
      "metadata": {},
      "source": [
        "### Create Standard PostgreSQL Indexes for Efficient Facet Searches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "807eb35d",
      "metadata": {},
      "outputs": [],
      "source": [
        "sql_array = []\n",
        "\n",
        "sql_array.append(\"DROP INDEX IF EXISTS idx_products_brand;\")\n",
        "sql_array.append(\"CREATE INDEX idx_products_brand ON products (brand);\")\n",
        "sql_array.append(\"DROP INDEX IF EXISTS idx_products_category;\")\n",
        "sql_array.append(\"CREATE INDEX idx_products_category ON products (category);\")\n",
        "sql_array.append(\"DROP INDEX IF EXISTS idx_products_retail_price;\")\n",
        "sql_array.append(\"CREATE INDEX idx_products_retail_price ON products (retail_price);\")\n",
        "sql_array.append(\"DROP INDEX IF EXISTS idx_products_sku;\")\n",
        "sql_array.append(\"CREATE INDEX idx_products_sku ON products (sku);\")\n",
        "\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf09c189",
      "metadata": {},
      "source": [
        "### Create ScaNN Indexes for Efficient ANN Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7a363d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "sql_array = []\n",
        "\n",
        "sql_array.append(\"CREATE EXTENSION IF NOT EXISTS alloydb_scann\")\n",
        "sql_array.append(\"SET SESSION scann.num_leaves_to_search = 1\")\n",
        "sql_array.append(\"SET SESSION scann.pre_reordering_num_neighbors=50\")\n",
        "sql_array.append(\"DROP INDEX IF EXISTS embedding_scann\")\n",
        "sql_array.append(\"\"\"\n",
        "CREATE INDEX embedding_scann ON products\n",
        "  USING scann (embedding cosine)\n",
        "  WITH (num_leaves=2);\n",
        "\"\"\")\n",
        "sql_array.append(\"DROP INDEX IF EXISTS product_description_embedding_scann\")\n",
        "sql_array.append(\"\"\"\n",
        "CREATE INDEX product_description_embedding_scann ON products\n",
        "  USING scann (product_description_embedding cosine)\n",
        "  WITH (num_leaves=2);\n",
        "\"\"\")\n",
        "sql_array.append(\"DROP INDEX IF EXISTS product_image_embedding_scann\")\n",
        "sql_array.append(\"\"\"\n",
        "CREATE INDEX product_image_embedding_scann ON products\n",
        "  USING scann (product_image_embedding cosine)\n",
        "  WITH (num_leaves=2);\n",
        "\"\"\")\n",
        "\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c47621a",
      "metadata": {},
      "source": [
        "### Create HNSW Index for Efficient BM25 ANN Sparsevec Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c50f56d",
      "metadata": {},
      "outputs": [],
      "source": [
        "sql_array = []\n",
        "sql_array.append(\"DROP INDEX IF EXISTS sparse_embedding_hnsw\")\n",
        "sql_array.append(\"\"\"\n",
        "CREATE INDEX sparse_embedding_hnsw ON products\n",
        "  USING hnsw (sparse_embedding sparsevec_ip_ops)\n",
        "  WITH (m = 16, ef_construction = 64);\n",
        "\"\"\")\n",
        "sql_array.append(\"SET hnsw.ef_search = 100;\") # This is necessary for better recall with sparsevec\n",
        "\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29f92dc6",
      "metadata": {},
      "source": [
        "### Create GIN Index for Efficient Full-text Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6366fdac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ref: https://www.postgresql.org/docs/current/gin.html\n",
        "sql_array = []\n",
        "sql_array.append(\"DROP INDEX IF EXISTS products_fts_document_gin;\")\n",
        "sql_array.append(\"CREATE INDEX products_fts_document_gin ON products USING GIN (fts_document);\")\n",
        "\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "181a6f4c",
      "metadata": {},
      "source": [
        "### Enable The AlloyDB Columnar Engine\n",
        "\n",
        "References:\n",
        "* https://cloud.google.com/alloydb/docs/columnar-engine/configure\n",
        "* https://cloud.google.com/alloydb/docs/instance-configure-database-flags#gcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f11b655b",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = ! gcloud beta alloydb instances update {alloydb_instance} \\\n",
        "   --database-flags google_columnar_engine.enabled=on,google_columnar_engine.enable_vectorized_join=on,password.enforce_complexity=on,google_ml_integration.enable_model_support=on \\\n",
        "   --region={region} \\\n",
        "   --cluster={alloydb_cluster} \\\n",
        "   --project={project_id} \\\n",
        "   --update-mode=FORCE_APPLY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "628ebd9c",
      "metadata": {},
      "source": [
        "### Add Columns to Column Store Automatically\n",
        "\n",
        "Reference: https://cloud.google.com/alloydb/docs/columnar-engine/manage-content-recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c8e2b4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "sql = \"SELECT google_columnar_engine_recommend();\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "898e3f33",
      "metadata": {},
      "source": [
        "### View Recommended Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4abeff05",
      "metadata": {},
      "outputs": [],
      "source": [
        "sql = \"SELECT database_name, schema_name, relation_name, column_name FROM g_columnar_recommended_columns;\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53737633",
      "metadata": {},
      "source": [
        "### Validate Columns are Added to Columnar Engine\n",
        "\n",
        "Reference: https://cloud.google.com/alloydb/docs/columnar-engine/monitor-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c2950d",
      "metadata": {},
      "outputs": [],
      "source": [
        "sql = \"SELECT * FROM g_columnar_columns;\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b541c313",
      "metadata": {},
      "source": [
        "## Deploy Demo UI\n",
        "\n",
        "Clone the [UI Repo](https://github.com/paulramsey/snippets) and follow the [instructions](https://github.com/paulramsey/snippets/tree/main/cymbal-shops-alloydb/demo_app) to deploy the demo app UI for StyleSearch."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "cymbal_shops_stylesearch_demo",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
