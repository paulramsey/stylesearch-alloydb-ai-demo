{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "XJ1fNT-8xEQx",
      "metadata": {
        "id": "XJ1fNT-8xEQx"
      },
      "source": [
        "# Multimodal Hybrid Product Search on AlloyDB - With Data Prep\n",
        "\n",
        "This notebook provides a step-by-step example of implementing Hybrid Search in [AlloyDB for PostgreSQL](https://cloud.google.com/products/alloydb?e=48754805&hl=en) for Cymbal Shops, a fictional retailer with a large eCommerce presence. It combines multimodal vector embeddings ([`multimodalembedding@001`](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings)), fulltext search ([Generalized Inverted Index](https://www.postgresql.org/docs/current/gin.html)), and [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) sparse embeddings ([pgvector 0.7.0+](https://github.com/pgvector/pgvector?tab=readme-ov-file#sparse-vectors)) with [Reciprocal Rank Fusion](https://medium.com/@devalshah1619/mathematical-intuition-behind-reciprocal-rank-fusion-rrf-explained-in-2-mins-002df0cc5e2a) re-ranking for enhanced product search.\n",
        "\n",
        "> **IMPORTANT:** This notebook leverages Preview features in AlloyDB AI. Create your AlloyDB cluster (see below), then **[Sign up for the preview](https://docs.google.com/forms/d/e/1FAIpQLSfJ9vHIJ79nI7JWBDELPFL75pDQa4XVZQ2fxShfYddW0RwmLw/viewform)** to take full advantage of the features in this notebook. Use `ecom` as the database name in your request (you will create this database later).\n",
        "\n",
        "\n",
        "The high-level flow is as follows:\n",
        "- Imports a sample retail dataset (based on [theLook eCommerce dataset](https://console.cloud.google.com/marketplace/product/bigquery-public-data/thelook-ecommerce)) into an AlloyDB cluster.\n",
        "- Asynchronously generates product descriptions for 29,120 products using the Gemini 2.0 Flash model.\n",
        "- Asynchronously generates product images for 29,120 products using the Imagen 3 model.\n",
        "- Asynchronously generates multimodal embeddings for the product images and product descriptions using the `multimodalembedding@001` model.\n",
        "- Generates sparse embeddings for products using BM25.\n",
        "- Creates [ScaNN vector indexes](https://cloud.google.com/blog/products/databases/understanding-the-scann-index-in-alloydb) for fast dense vector embedding queries.\n",
        "- Creates an [HNSW](https://github.com/pgvector/pgvector?tab=readme-ov-file#hnsw) index for efficient sparse vector embedding queries.\n",
        "- Creates a [GIN index](https://www.postgresql.org/docs/current/gin.html) for fast full-text search.\n",
        "- Demonstrates a variety of search techniques (Traditional SQL, Dense Vector Search, Full-text Search, BM25 Sparse Vector Search, and Multimodal Search).\n",
        "- Demonstrates combining techniques with blended hybrid search (Vector + FTS + BM25 + Traditional SQL)\n",
        "\n",
        "To Do:\n",
        "- Fix multimodal text embeddings (image embeddings work, but multimodal text embeddings are inconsistent).\n",
        "- Add Search by Video.\n",
        "- Add Semantic Ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TduOO0v1Z8Zw",
      "metadata": {
        "id": "TduOO0v1Z8Zw"
      },
      "source": [
        "## Basic Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gn8g7-wCyZU6",
      "metadata": {
        "id": "Gn8g7-wCyZU6"
      },
      "source": [
        "You will need an AlloyDB for PostgreSQL instance to use this notebook. Create one now if you have not already created it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AJ1QMVgDWkvP",
      "metadata": {
        "id": "AJ1QMVgDWkvP"
      },
      "source": [
        "### Define Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zrQSpdwg1rwJweglQjYzUhDo",
      "metadata": {
        "id": "zrQSpdwg1rwJweglQjYzUhDo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Update these variables to match your environment\n",
        "project_id = \"your-project\"  # @param {type:\"string\"}\n",
        "region = \"your-region\"  # @param {type:\"string\"}\n",
        "vpc = \"your-vpc\"  # @param {type:\"string\"}\n",
        "image_bucket = \"your-bucket\"  # @param {type:\"string\"}\n",
        "index_bucket = \"your-bucket\"  # @param {type:\"string\"}\n",
        "export_bucket = \"your-bucket\"  # @param {type:\"string\"}\n",
        "alloydb_cluster = \"your-alloydb-cluster\"  # @param {type:\"string\"}\n",
        "alloydb_instance = \"your-alloydb-instance\"  # @param {type:\"string\"}\n",
        "alloydb_password = input(\"Please provide a password to be used for 'postgres' database user: \")\n",
        "\n",
        "# Don't change values below this line.\n",
        "alloydb_database = \"ecom\" \n",
        "database_backup_uri = \"gs://pr-public-demo-data/alloydb-retail-demo/data/ecom.sql\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IX5fm685HuB3",
      "metadata": {
        "id": "IX5fm685HuB3"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kv16h5fhHt7m",
      "metadata": {
        "id": "kv16h5fhHt7m"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet google-cloud-storage==2.19.0 \\\n",
        "                      google-cloud-aiplatform==1.74.0 \\\n",
        "                      pymilvus.model==0.3.2 \\\n",
        "                      asyncpg==0.30.0 \\\n",
        "                      google.cloud.alloydb.connector==1.9.0 \\\n",
        "                      jupyter-server==1.24.0 \\\n",
        "                      google-genai==1.4.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UCiNGP1Qxd6x",
      "metadata": {
        "id": "UCiNGP1Qxd6x"
      },
      "source": [
        "### Connect Your Google Cloud Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SLUGlG6UE2CK",
      "metadata": {
        "id": "SLUGlG6UE2CK"
      },
      "outputs": [],
      "source": [
        "# Configure gcloud.\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oIk5GxbnFaE3",
      "metadata": {
        "id": "oIk5GxbnFaE3"
      },
      "source": [
        "### Configure Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wvYGGRRoFXl4",
      "metadata": {
        "id": "wvYGGRRoFXl4"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "# Configure the root logger to output messages with INFO level or above\n",
        "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='%(asctime)s[%(levelname)5s][%(name)14s] - %(message)s',  datefmt='%H:%M:%S', force=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O-oqMC5Ox-ZM",
      "metadata": {
        "id": "O-oqMC5Ox-ZM"
      },
      "source": [
        "### Enable APIs for AlloyDB, Vertex AI, and Discovery Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X-bzfFb4A-xK",
      "metadata": {
        "id": "X-bzfFb4A-xK"
      },
      "source": [
        "You will need to enable these APIs in order to create an AlloyDB database and utilize Vertex AI as an embeddings service!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CKWrwyfzyTwH",
      "metadata": {
        "id": "CKWrwyfzyTwH"
      },
      "outputs": [],
      "source": [
        "!gcloud services enable alloydb.googleapis.com aiplatform.googleapis.com discoveryengine.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1LjmZwNSKd1i",
      "metadata": {
        "id": "1LjmZwNSKd1i"
      },
      "source": [
        "### Initialize GenAI Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8H0WLEVHKdsZ",
      "metadata": {
        "id": "8H0WLEVHKdsZ"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "genai_client = genai.Client(\n",
        "    vertexai=True, project=project_id, location=region\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GoH1gbi_jEiJ",
      "metadata": {
        "id": "GoH1gbi_jEiJ"
      },
      "source": [
        "## Define Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cg0RdzTkIhlh",
      "metadata": {
        "id": "cg0RdzTkIhlh"
      },
      "source": [
        "#### rest_api_helper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "st4dRKZklRc4",
      "metadata": {
        "id": "st4dRKZklRc4"
      },
      "outputs": [],
      "source": [
        "from google.auth.transport import requests\n",
        "import google.auth.transport.requests\n",
        "import requests\n",
        "import google.auth\n",
        "import json\n",
        "\n",
        "# Get an access token based upon the current user\n",
        "creds, _ = google.auth.default()\n",
        "authed_session = google.auth.transport.requests.AuthorizedSession(creds)\n",
        "access_token=creds.token\n",
        "\n",
        "if project_id:\n",
        "  authed_session.headers.update({\"x-goog-user-project\": project_id}) # Required to workaround a project quota bug\n",
        "\n",
        "def rest_api_helper(\n",
        "    session: requests.Session,\n",
        "    url: str,\n",
        "    http_verb: str,\n",
        "    request_body: dict = None,\n",
        "    params: dict = None\n",
        "  ) -> dict:\n",
        "  \"\"\"Calls a REST API using a pre-authenticated requests Session.\"\"\"\n",
        "\n",
        "  headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "  try:\n",
        "\n",
        "    if http_verb == \"GET\":\n",
        "      response = session.get(url, headers=headers, params=params)\n",
        "    elif http_verb == \"POST\":\n",
        "      response = session.post(url, json=request_body, headers=headers, params=params)\n",
        "    elif http_verb == \"PUT\":\n",
        "      response = session.put(url, json=request_body, headers=headers, params=params)\n",
        "    elif http_verb == \"PATCH\":\n",
        "      response = session.patch(url, json=request_body, headers=headers, params=params)\n",
        "    elif http_verb == \"DELETE\":\n",
        "      response = session.delete(url, headers=headers, params=params)\n",
        "    else:\n",
        "      raise ValueError(f\"Unknown HTTP verb: {http_verb}\")\n",
        "\n",
        "    # Raise an exception for bad status codes (4xx or 5xx)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Check if response has content before trying to parse JSON\n",
        "    if response.content:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return {} # Return empty dict for empty responses (like 204 No Content)\n",
        "\n",
        "  except requests.exceptions.RequestException as e:\n",
        "      # Catch potential requests library errors (network, timeout, etc.)\n",
        "      # Log detailed error information\n",
        "      print(f\"Request failed: {e}\")\n",
        "      if e.response is not None:\n",
        "          print(f\"Request URL: {e.request.url}\")\n",
        "          print(f\"Request Headers: {e.request.headers}\")\n",
        "          print(f\"Request Body: {e.request.body}\")\n",
        "          print(f\"Response Status: {e.response.status_code}\")\n",
        "          print(f\"Response Text: {e.response.text}\")\n",
        "          # Re-raise a more specific error or a custom one\n",
        "          raise RuntimeError(f\"API call failed with status {e.response.status_code}: {e.response.text}\") from e\n",
        "      else:\n",
        "          raise RuntimeError(f\"API call failed: {e}\") from e\n",
        "  except json.JSONDecodeError as e:\n",
        "      print(f\"Failed to decode JSON response: {e}\")\n",
        "      print(f\"Response Text: {response.text}\")\n",
        "      raise RuntimeError(f\"Invalid JSON received from API: {response.text}\") from e\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ol_OBUX4Lr4m",
      "metadata": {
        "id": "Ol_OBUX4Lr4m"
      },
      "source": [
        "#### run_query()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rLn6HEQb85DQ",
      "metadata": {
        "id": "rLn6HEQb85DQ"
      },
      "outputs": [],
      "source": [
        "# Create AlloyDB Query Helper Function\n",
        "import sqlalchemy\n",
        "from sqlalchemy import text, exc\n",
        "import pandas as pd\n",
        "\n",
        "async def run_query(pool, sql: str, params = None, output_as_df: bool = True):\n",
        "    \"\"\"Executes a SQL query or statement against the database pool.\n",
        "\n",
        "    Handles various SQL statements:\n",
        "    - SELECT/WITH: Returns results as a DataFrame (if output_as_df=True)\n",
        "      or ResultProxy. Supports parameters. Does not commit.\n",
        "    - EXPLAIN/EXPLAIN ANALYZE: Executes the explain, returns the query plan\n",
        "      as a formatted multi-line string. Ignores output_as_df.\n",
        "      Supports parameters. Does not commit.\n",
        "    - INSERT/UPDATE/DELETE/CREATE/ALTER etc.: Executes the statement,\n",
        "      commits the transaction, logs info, and returns the ResultProxy.\n",
        "      Supports single or bulk parameters (executemany).\n",
        "\n",
        "    Args:\n",
        "      pool: An asynchronous SQLAlchemy connection pool.\n",
        "      sql: A string containing the SQL query or statement template.\n",
        "      params: Optional.\n",
        "        - None: Execute raw SQL (Use with caution for non-SELECT/EXPLAIN).\n",
        "        - dict or tuple: Parameters for a single execution.\n",
        "        - list of dicts/tuples: Parameters for bulk execution (executemany).\n",
        "      output_as_df (bool): If True and query is SELECT/WITH, return pandas DataFrame.\n",
        "                           Ignored for EXPLAIN and non-data-returning statements.\n",
        "\n",
        "    Returns:\n",
        "      pandas.DataFrame | str | sqlalchemy.engine.Result | None:\n",
        "        - DataFrame: For SELECT/WITH if output_as_df=True.\n",
        "        - str: For EXPLAIN/EXPLAIN ANALYZE, containing the formatted query plan.\n",
        "        - ResultProxy: For non-SELECT/WITH/EXPLAIN statements, or SELECT/WITH\n",
        "                       if output_as_df=False.\n",
        "        - None: If a SQLAlchemy ProgrammingError or other specific error occurs.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Catches and logs `sqlalchemy.exc.ProgrammingError`, returning None.\n",
        "                   May re-raise other database exceptions.\n",
        "\n",
        "    Example Execution:\n",
        "      Single SELECT:\n",
        "        sql_select = \"SELECT ticker, company_name from investments LIMIT 5\"\n",
        "        df_result = await run_query(pool, sql_select)\n",
        "\n",
        "      Single non-SELECT - Parameterized (Safe!):\n",
        "        Parameterized INSERT:\n",
        "          sql_insert = \"INSERT INTO investments (ticker, company_name) VALUES (:ticker, :name)\"\n",
        "          params_insert = {\"ticker\": \"NEW\", \"name\": \"New Company\"}\n",
        "          insert_result = await run_query(pool, sql_insert, params_insert)\n",
        "\n",
        "        Parameterized UPDATE:\n",
        "          sql_update = \"UPDATE products SET price = :price WHERE id = :product_id\"\n",
        "          params_update = {\"price\": 99.99, \"product_id\": 123}\n",
        "          update_result = await run_query(pool, sql_update, params_update)\n",
        "\n",
        "      Bulk Update:\n",
        "        docs = pd.DataFrame([\n",
        "            {'id': 101, 'sparse_embedding': '[0.1, 0.2]'},\n",
        "            {'id': 102, 'sparse_embedding': '[0.3, 0.4]'},\n",
        "            # ... more rows\n",
        "        ])\n",
        "\n",
        "        update_sql_template = '''\n",
        "            UPDATE products\n",
        "            SET sparse_embedding = :embedding,\n",
        "                sparse_embedding_model = 'BM25'\n",
        "            WHERE id = :product_id\n",
        "        ''' # Using named parameters :param_name\n",
        "\n",
        "        # Prepare list of dictionaries for params\n",
        "        data_to_update = [\n",
        "            {\"embedding\": row.sparse_embedding, \"product_id\": row.id}\n",
        "            for row in docs.itertuples(index=False)\n",
        "        ]\n",
        "\n",
        "        if data_to_update:\n",
        "          bulk_result = await run_query(pool, update_sql_template, data_to_update)\n",
        "          # bulk_result is the SQLAlchemy ResultProxy\n",
        "\n",
        "    \"\"\"\n",
        "    sql_lower_stripped = sql.strip().lower()\n",
        "    is_select_with = sql_lower_stripped.startswith(('select', 'with'))\n",
        "    is_explain = sql_lower_stripped.startswith('explain')\n",
        "\n",
        "    # Determine if the statement is expected to return data rows or a plan\n",
        "    is_data_returning = is_select_with or is_explain\n",
        "\n",
        "    # Determine actual DataFrame output eligibility (only for SELECT/WITH)\n",
        "    effective_output_as_df = output_as_df and is_select_with\n",
        "\n",
        "    # Check if params suggest a bulk operation (for logging purposes)\n",
        "    is_bulk_operation = isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], (dict, tuple, list))\n",
        "\n",
        "    async with pool.connect() as conn:\n",
        "        try:\n",
        "          # Execute with or without params\n",
        "          if params:\n",
        "              result = await conn.execute(text(sql), params)\n",
        "          else:\n",
        "              # Add warning for raw SQL only if it's NOT data-returning\n",
        "              #if not is_data_returning:\n",
        "                  #logging.warning(\"Executing non-SELECT/EXPLAIN raw SQL without parameters. Ensure SQL is safe.\")\n",
        "              result = await conn.execute(text(sql))\n",
        "\n",
        "          # --- Handle statements that return data or plan ---\n",
        "          if is_data_returning:\n",
        "              if is_explain:\n",
        "                  # Fetch and format EXPLAIN output as a string\n",
        "                    try:\n",
        "                        plan_rows = result.fetchall()\n",
        "                        # EXPLAIN output is usually text in the first column\n",
        "                        query_plan = \"\\n\".join([str(row[0]) for row in plan_rows])\n",
        "                        #logging.info(f\"EXPLAIN executed successfully for: {sql[:100]}...\")\n",
        "                        return query_plan\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error fetching/formatting EXPLAIN result: {e}\")\n",
        "                        return None\n",
        "              else: # Handle SELECT / WITH\n",
        "                  if effective_output_as_df:\n",
        "                      try:\n",
        "                          rows = result.fetchall()\n",
        "                          column_names = result.keys()\n",
        "                          df = pd.DataFrame(rows, columns=column_names)\n",
        "                          #logging.info(f\"SELECT/WITH executed successfully, returning DataFrame for: {sql[:100]}...\")\n",
        "                          return df\n",
        "                      except Exception as e:\n",
        "                          logging.error(f\"Error converting SELECT result to DataFrame: {e}\")\n",
        "                          logging.info(f\"Returning raw ResultProxy for SELECT/WITH due to DataFrame conversion error for: {sql[:100]}...\")\n",
        "                          return result # Fallback to raw result\n",
        "                  else:\n",
        "                      # Return raw result proxy for SELECT/WITH if df output not requested\n",
        "                      #logging.info(f\"SELECT/WITH executed successfully, returning ResultProxy for: {sql[:100]}...\")\n",
        "                      return result\n",
        "\n",
        "          # --- Handle Non-Data Returning Statements (INSERT, UPDATE, DELETE, CREATE, etc.) ---\n",
        "          else:\n",
        "              await conn.commit() # Commit changes ONLY for these statements\n",
        "              operation_type = sql.strip().split()[0].upper()\n",
        "              row_count = result.rowcount # Note: rowcount behavior varies\n",
        "\n",
        "              if is_bulk_operation:\n",
        "                  print(f\"Bulk {operation_type} executed for {len(params)} items. Result rowcount: {row_count}\")\n",
        "              elif operation_type in ['INSERT', 'UPDATE', 'DELETE']:\n",
        "                  print(f\"{operation_type} statement executed successfully. {row_count} row(s) affected.\")\n",
        "              else: # CREATE, ALTER, etc.\n",
        "                  print(f\"{operation_type} statement executed successfully. Result rowcount: {row_count}\")\n",
        "              return result # Return the result proxy\n",
        "\n",
        "        except exc.ProgrammingError as e:\n",
        "            # Log the error with context\n",
        "            logging.error(f\"SQL Programming Error executing query:\\nSQL: {sql[:500]}...\\nParams (sample): {str(params)[:500]}...\\nError: {e}\")\n",
        "            # Rollback might happen automatically on context exit with error, but explicit can be clearer\n",
        "            # await conn.rollback() # Consider if needed based on pool/transaction settings\n",
        "            return None # Return None on handled programming errors\n",
        "        except Exception as e:\n",
        "            # Log other unexpected errors\n",
        "            logging.error(f\"An unexpected error occurred during query execution:\\nSQL: {sql[:500]}...\\nError: {e}\")\n",
        "            # await conn.rollback() # Consider if needed\n",
        "            raise # Re-raise unexpected errors\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42dpYZ-Ffa1o",
      "metadata": {
        "id": "42dpYZ-Ffa1o"
      },
      "source": [
        "### retry_condition()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MUPmSPSYfasF",
      "metadata": {
        "id": "MUPmSPSYfasF"
      },
      "outputs": [],
      "source": [
        "from tenacity import retry, wait_exponential, stop_after_attempt, before_sleep_log, retry_if_exception, wait_fixed\n",
        "\n",
        "def retry_condition(error):\n",
        "  error_string = str(error)\n",
        "  print(error_string)\n",
        "\n",
        "  retry_errors = [\n",
        "      \"429 Quota exceeded\",\n",
        "      #\"The prompt could not be submitted\",\n",
        "  ]\n",
        "\n",
        "  for retry_error in retry_errors:\n",
        "    if retry_error in error_string:\n",
        "      print(\"Retrying...\")\n",
        "      return True\n",
        "\n",
        "  return False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BThL3CqUIohi",
      "metadata": {
        "id": "BThL3CqUIohi"
      },
      "source": [
        "### async_generate_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rOB2AotIHNrD",
      "metadata": {
        "id": "rOB2AotIHNrD"
      },
      "outputs": [],
      "source": [
        "async def async_generate_text(prompt):\n",
        "  result = await genai_client.aio.models.generate_content(\n",
        "                model='gemini-2.0-flash',\n",
        "                contents=[prompt]\n",
        "            )\n",
        "  return result.candidates[0].content.parts[0].text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Pu2U9b4-KH2f",
      "metadata": {
        "id": "Pu2U9b4-KH2f"
      },
      "source": [
        "### sync_generate_text()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QXixftyNKHkw",
      "metadata": {
        "id": "QXixftyNKHkw"
      },
      "outputs": [],
      "source": [
        "def sync_generate_text(prompt):\n",
        "    result = genai_client.models.generate_content(\n",
        "                  model='gemini-2.0-flash',\n",
        "                  contents=[prompt]\n",
        "              )\n",
        "    return result.candidates[0].content.parts[0].text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mz01QPlXvi99",
      "metadata": {
        "id": "Mz01QPlXvi99"
      },
      "source": [
        "### async_generate_embedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7mTSXqYxviX6",
      "metadata": {
        "id": "7mTSXqYxviX6"
      },
      "outputs": [],
      "source": [
        "async def async_generate_embedding(input):\n",
        "  result = await genai_client.aio.models.embed_content(\n",
        "    model='gemini-embedding-001',\n",
        "    contents=input\n",
        "  )\n",
        "  return result.embeddings[0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2iCqeWOg0F-a",
      "metadata": {
        "id": "2iCqeWOg0F-a"
      },
      "source": [
        "### generate_image()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SlE6KQxQ0F-b",
      "metadata": {
        "id": "SlE6KQxQ0F-b"
      },
      "outputs": [],
      "source": [
        "# Reference: https://cloud.google.com/vertex-ai/generative-ai/docs/image/generate-images\n",
        "#            https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/imagen-api\n",
        "\n",
        "import os\n",
        "import vertexai\n",
        "from google.cloud import storage\n",
        "from vertexai.preview.vision_models import ImageGenerationModel\n",
        "\n",
        "vertexai.init(project=project_id, location=region)\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(image_bucket)\n",
        "\n",
        "imagen_model = ImageGenerationModel.from_pretrained(\"imagen-3.0-fast-generate-001\")\n",
        "\n",
        "@retry(wait=wait_exponential(multiplier=1, min=1, max=10), stop=stop_after_attempt(2), retry=retry_if_exception(retry_condition), before_sleep=before_sleep_log(logging.getLogger(), logging.INFO))\n",
        "def generate_image(prompt, product_sku, id):\n",
        "\n",
        "    image_name = f\"{product_sku}.png\"\n",
        "    destination_blob_name = f\"product-images/{image_name}\"\n",
        "    logging.info(f\"Generating image for: {image_name})\")\n",
        "\n",
        "    images = imagen_model.generate_images(\n",
        "        prompt=prompt,\n",
        "        number_of_images=1,\n",
        "        language=\"en\",\n",
        "        add_watermark=True,\n",
        "        aspect_ratio=\"1:1\",\n",
        "        safety_filter_level=\"block_some\",\n",
        "        person_generation=\"dont_allow\",\n",
        "    )\n",
        "\n",
        "    if not images.images:\n",
        "      logging.info(f\"RETRY 1: Retrying with a different prompt for {image_name}.\")\n",
        "      rewritten_prompt = sync_generate_text(f\"Responding in 1 sentence, simplify this prompt for Imagen3: {prompt}\")\n",
        "      logging.info(f\"Modified prompt for id {id} {image_name}: {rewritten_prompt}\")\n",
        "\n",
        "      images = imagen_model.generate_images(\n",
        "          prompt=rewritten_prompt,\n",
        "          number_of_images=1,\n",
        "          language=\"en\",\n",
        "          add_watermark=True,\n",
        "          aspect_ratio=\"1:1\",\n",
        "          safety_filter_level=\"block_only_high\",\n",
        "          person_generation=\"dont_allow\",\n",
        "      )\n",
        "\n",
        "      if not images.images:\n",
        "        logging.warning(f\"FAILED: Image generation failed for {image_name}. Prompt: {prompt}\")\n",
        "        return None\n",
        "\n",
        "    #logging.info(f\"Done generating image for: {image_name})\")\n",
        "\n",
        "    # Write the image locally\n",
        "    #logging.info(f\"Writing image locally: {image_name})\")\n",
        "    local_filename = f\"{image_name}\"\n",
        "    images[0].save(location=local_filename, include_generation_parameters=False)\n",
        "\n",
        "    # Upload the image to GCS\n",
        "    #logging.info(f\"Uploading to GCS: {image_name})\")\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(local_filename, content_type='image/png')\n",
        "\n",
        "\n",
        "    # Clean up the local file\n",
        "    #logging.info(f\"Removing the local file: {image_name})\")\n",
        "    os.remove(local_filename)\n",
        "\n",
        "    #logging.info(f\"Returning URI: {image_name})\")\n",
        "    return f\"gs://{image_bucket}/{destination_blob_name}\" # Return GCS URI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rxtHPIChMNl-",
      "metadata": {
        "id": "rxtHPIChMNl-"
      },
      "source": [
        "### generate_multimodal_embeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K2BD28t6Lv-s",
      "metadata": {
        "id": "K2BD28t6Lv-s"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.vision_models import Image as vai_image, MultiModalEmbeddingModel # Import Image as vai_image to avoid collions with PIL Image\n",
        "\n",
        "vertexai.init(project=project_id, location=region)\n",
        "mme_model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding@001\")\n",
        "\n",
        "@retry(wait=wait_fixed(10), stop=stop_after_attempt(7), retry=retry_if_exception(retry_condition))\n",
        "def generate_multimodal_embeddings(uri, text):\n",
        "\n",
        "  # If no image is provided, just generate text embeddings.\n",
        "  if uri is None:\n",
        "      embeddings = mme_model.get_embeddings(\n",
        "          contextual_text=text,\n",
        "          dimension=1408,\n",
        "      )\n",
        "      return embeddings\n",
        "\n",
        "  # Load image\n",
        "  image = vai_image.load_from_file(f\"{uri}\")\n",
        "\n",
        "  # If no text is provided, just generate image embeddings\n",
        "  if text is None:\n",
        "      embeddings = mme_model.get_embeddings(\n",
        "          image=image,\n",
        "          dimension=1408,\n",
        "      )\n",
        "      return embeddings\n",
        "\n",
        "  # If image and text are provided, generate both text and image embeddings\n",
        "  embeddings = mme_model.get_embeddings(\n",
        "      image=image,\n",
        "      contextual_text=text,\n",
        "      dimension=1408,\n",
        "  )\n",
        "  return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LyO-ljPWynPL",
      "metadata": {
        "id": "LyO-ljPWynPL"
      },
      "source": [
        "### df_image_formatter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LHtHNhdGynHd",
      "metadata": {
        "id": "LHtHNhdGynHd"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from PIL import Image\n",
        "\n",
        "def df_image_formatter(path, width = 200):\n",
        "    \"\"\"\n",
        "    Formats an image path into an HTML image tag.\n",
        "    \"\"\"\n",
        "    path = path.replace(f\"gs://{image_bucket}\",f\"https://storage.cloud.google.com/{image_bucket}\")\n",
        "    return f\"<img src='{path}' width={width}>\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc71e447",
      "metadata": {
        "id": "bc71e447"
      },
      "source": [
        "## OPTIONAL: Create an AlloyDB Cluster\n",
        "\n",
        "You will need an AlloyDB for PostgreSQL cluster to use this notebook. If you already have an AlloyDB cluster, you can skip to the `Connect to the AlloyDB Cluster` section. Otherwise, use the cells below to create one.\n",
        "\n",
        "> ⏳ - Creating an AlloyDB cluster may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb44660b",
      "metadata": {
        "id": "cb44660b"
      },
      "outputs": [],
      "source": [
        "# create the AlloyDB Cluster\n",
        "!gcloud beta alloydb clusters create {alloydb_cluster} --password={alloydb_password} --region={region}\n",
        "\n",
        "# Create the AlloyDB Instance\n",
        "!gcloud beta alloydb instances create {alloydb_instance} --instance-type=PRIMARY --cpu-count=2 --region={region} --cluster={alloydb_cluster}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91719891",
      "metadata": {
        "id": "91719891"
      },
      "source": [
        "To connect to your AlloyDB instance from this notebook, you will need to enable public IP on your instance. Alternatively, you can follow [these instructions](https://cloud.google.com/alloydb/docs/connect-external) to connect to an AlloyDB for PostgreSQL instance with Private IP from outside your VPC. You can also use the `--authorized-external-networks` flag to limit communication over public IP to specific IP address ranges if desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1cf6629",
      "metadata": {
        "id": "d1cf6629"
      },
      "outputs": [],
      "source": [
        "# Enable Public IP on AlloyDB\n",
        "!gcloud beta alloydb instances update {instance_name} --region={region} --cluster={cluster_name} --assign-inbound-public-ip=ASSIGN_IPV4 --database-flags=\"password.enforce_complexity=on\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e4ca8d",
      "metadata": {
        "id": "07e4ca8d"
      },
      "source": [
        "## Connect to AlloyDB Cluster\n",
        "\n",
        "This function will create a connection pool to your AlloyDB instance using the AlloyDB Python connector. The AlloyDB Python connector will automatically create secure connections to your AlloyDB instance using mTLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77eeb8b5",
      "metadata": {
        "id": "77eeb8b5"
      },
      "outputs": [],
      "source": [
        "import asyncpg\n",
        "\n",
        "import sqlalchemy\n",
        "from sqlalchemy.ext.asyncio import AsyncEngine, create_async_engine\n",
        "\n",
        "from google.cloud.alloydb.connector import AsyncConnector, IPTypes\n",
        "\n",
        "async def init_connection_pool(connector: AsyncConnector, db_name: str = alloydb_database, pool_size: int = 5) -> AsyncEngine:\n",
        "    # initialize Connector object for connections to AlloyDB\n",
        "    connection_string = f\"projects/{project_id}/locations/{region}/clusters/{alloydb_cluster}/instances/{alloydb_instance}\"\n",
        "\n",
        "    async def getconn() -> asyncpg.Connection:\n",
        "        conn: asyncpg.Connection = await connector.connect(\n",
        "            connection_string,\n",
        "            \"asyncpg\",\n",
        "            user=\"postgres\",\n",
        "            password=alloydb_password,\n",
        "            db=db_name,\n",
        "            ip_type=IPTypes.PRIVATE, # Optionally use IPTypes.PUBLIC\n",
        "        )\n",
        "        return conn\n",
        "\n",
        "    pool = create_async_engine(\n",
        "        \"postgresql+asyncpg://\",\n",
        "        async_creator=getconn,\n",
        "        pool_size=pool_size,\n",
        "        max_overflow=0,\n",
        "        isolation_level='AUTOCOMMIT'\n",
        "    )\n",
        "    return pool\n",
        "\n",
        "connector = AsyncConnector()\n",
        "\n",
        "postgres_db_pool = await init_connection_pool(connector, \"postgres\")\n",
        "ecom_db_pool = await init_connection_pool(connector, f\"{alloydb_database}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3vrrT_n4MQ9I",
      "metadata": {
        "id": "3vrrT_n4MQ9I"
      },
      "source": [
        "## Import Sample Data to AlloyDB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WzevAboCf1xx",
      "metadata": {
        "id": "WzevAboCf1xx"
      },
      "source": [
        "### Add Required Permissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OKu9MWhzid30",
      "metadata": {
        "id": "OKu9MWhzid30"
      },
      "outputs": [],
      "source": [
        "project_number = ! gcloud projects describe {project_id} --format='value(projectNumber)'\n",
        "project_number = project_number[0]\n",
        "\n",
        "# These permissions are required to read from GCS for the data import and to integrate with Vertex AI for on-the-fly embedding generation.\n",
        "roles_array = [\n",
        "    \"roles/storage.admin\",\n",
        "    \"roles/aiplatform.user\",\n",
        "    \"roles/discoveryengine.admin\",\n",
        "    \"roles/secretmanager.secretAccessor\",\n",
        "]\n",
        "\n",
        "for r in roles_array:\n",
        "  ! gcloud projects add-iam-policy-binding {project_id} \\\n",
        "      --member=\"serviceAccount:service-{project_number}@gcp-sa-alloydb.iam.gserviceaccount.com\" \\\n",
        "      --role=\"{r}\"\n",
        "\n",
        "# These permissions are required to generate a token later for semantic ranking models\n",
        "roles_array = [\n",
        "    \"roles/iam.serviceAccountTokenCreator\"\n",
        "]\n",
        "\n",
        "current_user = ! gcloud auth list --format='value(account)'\n",
        "current_user = current_user[0]\n",
        "\n",
        "for r in roles_array:\n",
        "  ! gcloud projects add-iam-policy-binding {project_id} \\\n",
        "      --member=\"user:{current_user}\" \\\n",
        "      --role=\"{r}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "und--znqy0Ic",
      "metadata": {
        "id": "und--znqy0Ic"
      },
      "source": [
        "### OPTIONAL: Drop Existing Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bp7H7X1ky0Bm",
      "metadata": {
        "id": "Bp7H7X1ky0Bm"
      },
      "outputs": [],
      "source": [
        "# Close existing connections to the database\n",
        "sql = f\"\"\"SELECT pg_terminate_backend(pg_stat_activity.pid)\n",
        "FROM pg_stat_activity\n",
        "WHERE pg_stat_activity.datname = '{alloydb_database}'\n",
        "  AND pid <> pg_backend_pid();\"\"\"\n",
        "await run_query(postgres_db_pool, sql)\n",
        "\n",
        "# Uncomment the lines below to drop an existing database before re-creating it\n",
        "#sql = f\"DROP DATABASE {alloydb_database};\"\n",
        "#await run_query(postgres_db_pool, sql)\n",
        "\n",
        "# Reinitiate the connection pool\n",
        "ecom_db_pool = await init_connection_pool(connector, f\"{alloydb_database}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TpeQdjJZ577L",
      "metadata": {
        "id": "TpeQdjJZ577L"
      },
      "source": [
        "### Create Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JGA7etBk568a",
      "metadata": {
        "id": "JGA7etBk568a"
      },
      "outputs": [],
      "source": [
        "# Create the database\n",
        "sql = f\"CREATE DATABASE {alloydb_database};\"\n",
        "await run_query(postgres_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50cj8jPA7F2z",
      "metadata": {
        "id": "50cj8jPA7F2z"
      },
      "source": [
        "### Install Pre-requisite Extensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J3y6BXLf7FwR",
      "metadata": {
        "id": "J3y6BXLf7FwR"
      },
      "outputs": [],
      "source": [
        "sql_array = []\n",
        "\n",
        "sql_array.append(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
        "\n",
        "sql_array.append(\"CREATE EXTENSION IF NOT EXISTS google_ml_integration;\")\n",
        "\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ufkdb1Iy3JJM",
      "metadata": {
        "id": "Ufkdb1Iy3JJM"
      },
      "source": [
        "### Run the Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56F76nif1pY",
      "metadata": {
        "id": "f56F76nif1pY"
      },
      "outputs": [],
      "source": [
        "# Reference: https://cloud.google.com/alloydb/docs/reference/rest/v1/projects.locations.clusters/import\n",
        "#            https://cloud.google.com/alloydb/docs/import-sql-file\n",
        "\n",
        "import time\n",
        "\n",
        "url = f\"https://alloydb.googleapis.com/v1/projects/{project_id}/locations/{region}/clusters/{alloydb_cluster}:import\"\n",
        "request_body = {\n",
        "   \"gcsUri\": f\"{database_backup_uri}\",\n",
        "   \"database\": f\"{alloydb_database}\",\n",
        "   \"user\": \"postgres\",\n",
        "   \"sqlImportOptions\": {}\n",
        "}\n",
        "\n",
        "result = rest_api_helper(authed_session, url, 'POST', request_body, {})\n",
        "print(f\"Kicked off import: {result}\")\n",
        "\n",
        "operation_id = result['name']\n",
        "\n",
        "operation_complete = False\n",
        "while operation_complete == False:\n",
        "  print(f\"Import still running: {operation_id}\")\n",
        "  url = f\"https://alloydb.googleapis.com/v1/{operation_id}\"\n",
        "  response = rest_api_helper(authed_session, url, 'GET', request_body, {})\n",
        "  operation_complete = response['done']\n",
        "  if operation_complete:\n",
        "    print(f\"Operation complete. Check result payload for potential errors. \\nResult: {response}\")\n",
        "    continue\n",
        "  time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qsc8Zsf2srYu",
      "metadata": {
        "id": "qsc8Zsf2srYu"
      },
      "source": [
        "### Check Row Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XyiXMrtzbrFz",
      "metadata": {
        "id": "XyiXMrtzbrFz"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"\n",
        "SELECT 'distribution_centers' AS table_name, (SELECT COUNT(*) FROM distribution_centers) AS actual_row_count, 10 AS target_row_count\n",
        "UNION ALL\n",
        "SELECT 'events', (SELECT COUNT(*) FROM events), 2438862\n",
        "UNION ALL\n",
        "SELECT 'inventory_items', (SELECT COUNT(*) FROM inventory_items), 494254\n",
        "UNION ALL\n",
        "SELECT 'orders', (SELECT COUNT(*) FROM orders), 125905\n",
        "UNION ALL\n",
        "SELECT 'order_items', (SELECT COUNT(*) FROM order_items), 182905\n",
        "UNION ALL\n",
        "SELECT 'products', (SELECT COUNT(*) FROM products), 29120\n",
        "UNION ALL\n",
        "SELECT 'users', (SELECT COUNT(*) FROM users), 100000;\n",
        "\"\"\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "as-XHQMY-YuP",
      "metadata": {
        "id": "as-XHQMY-YuP"
      },
      "source": [
        "### Test the Vertex AI Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RSZn7lfc5R3O",
      "metadata": {
        "id": "RSZn7lfc5R3O"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT embedding('gemini-embedding-001', 'This string will be transformed into an embedding.');\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1NgOphv_ghlg",
      "metadata": {
        "id": "1NgOphv_ghlg"
      },
      "source": [
        "## OPTIONAL: Remove Branding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "az0sPmtgg5i3",
      "metadata": {
        "id": "az0sPmtgg5i3"
      },
      "source": [
        "### Remove Brands from Product Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yiD5D697g9G_",
      "metadata": {
        "id": "yiD5D697g9G_"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT COUNT(*) FROM products WHERE name ILIKE CONCAT('%', brand, '%') \"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mPNcJYzlhuTR",
      "metadata": {
        "id": "mPNcJYzlhuTR"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"UPDATE products SET name = regexp_replace(name, brand, '!!BRAND!!', 'gi')\n",
        "WHERE name ILIKE '%' || brand || '%';\"\"\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UYULPMTFg1Mr",
      "metadata": {
        "id": "UYULPMTFg1Mr"
      },
      "source": [
        "### Create New Brand Names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l-Iz5ZsqjJ83",
      "metadata": {
        "id": "l-Iz5ZsqjJ83"
      },
      "source": [
        "#### Define Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZM15bwUojMJM",
      "metadata": {
        "id": "ZM15bwUojMJM"
      },
      "outputs": [],
      "source": [
        "def build_brand_prompt(brand):\n",
        "  prompt = f\"\"\"**Persona & Context:**\n",
        "You are a skilled Copywriter for Cymbal Shops. Cymbal Shops is a specialty big box retailer offering a diverse, curated mix of trendy clothing, unique household knick-knacks, stylish furnishings, and interesting personal items. Our customers appreciate value, style, and finding items with personality.\n",
        "\n",
        "**Task:**\n",
        "Create a new, completely original Brand Name analogous to the old Brand Name.\n",
        "\n",
        "**Output Requirements:**\n",
        "* **Length:** 1-3 words (approx. 3-5 sentences).\n",
        "* **Goal:** The new Brand Name should be analogous to the old Brand Name, but it should be entirely original.\n",
        "* **Format:** Text with NO formatting. Output ONLY the new Brand Name.\n",
        "* **Exclusions:** The new Brand Name MUST NOT be a well-known Brand Name that is already in use.\n",
        "\n",
        "**Product Information to Use:**\n",
        "Old Brand Name: {brand}\n",
        "\n",
        "**Now, come up with a new, completely original Brand Name.**\n",
        "\"\"\"\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M77x07MfkzL6",
      "metadata": {
        "id": "M77x07MfkzL6"
      },
      "source": [
        "#### Create Mapping Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vyFziZhBk92e",
      "metadata": {
        "id": "vyFziZhBk92e"
      },
      "outputs": [],
      "source": [
        "sql_array = []\n",
        "\n",
        "sql_array.append(\"DROP TABLE IF EXISTS brand_mapping;\")\n",
        "\n",
        "sql_array.append(\"\"\"\n",
        "CREATE TABLE brand_mapping\n",
        "AS\n",
        "SELECT DISTINCT brand FROM products;\n",
        "\"\"\")\n",
        "\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ux3ghS5llqoc",
      "metadata": {
        "id": "Ux3ghS5llqoc"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"\n",
        "ALTER TABLE brand_mapping\n",
        "ADD COLUMN new_brand TEXT\n",
        "\"\"\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dmRVobAakzF7",
      "metadata": {
        "id": "dmRVobAakzF7"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"SELECT brand FROM brand_mapping WHERE new_brand IS NULL;\"\"\"\n",
        "brand_df = await run_query(ecom_db_pool, sql)\n",
        "brand_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2Jh_EuQVkcSX",
      "metadata": {
        "id": "2Jh_EuQVkcSX"
      },
      "source": [
        "#### Build Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mLT03Sqvkgbs",
      "metadata": {
        "id": "mLT03Sqvkgbs"
      },
      "outputs": [],
      "source": [
        "brand_df['prompt'] = brand_df.apply(\n",
        "    lambda row: build_brand_prompt(\n",
        "        row['brand'],\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "brand_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xtSwf1F8kh1_",
      "metadata": {
        "id": "xtSwf1F8kh1_"
      },
      "source": [
        "#### Generate New Brands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZSyQqAuFgheY",
      "metadata": {
        "id": "ZSyQqAuFgheY"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "async def load_queue_from_dataframe(df: pd.DataFrame, queue: asyncio.Queue, num_consumers: int):\n",
        "    \"\"\"\n",
        "    Iterates through DataFrame rows and puts them into the asyncio queue.\n",
        "\n",
        "    Args:\n",
        "        df: The Pandas DataFrame to process.\n",
        "        queue: The asyncio.Queue to put items into.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Producer: Starting to load {len(df)} items into the queue...\")\n",
        "    # Use itertuples for efficiency. index=False avoids adding the DataFrame index.\n",
        "    # name=None uses default namedtuple name 'Pandas'\n",
        "    for row_tuple in df.itertuples(index=False, name='Product'):\n",
        "        # Convert the named tuple to a dictionary - often easier for consumers\n",
        "        item = row_tuple._asdict()\n",
        "        await queue.put(item)\n",
        "        logging.info(f\"Producer: Put item {item.get('sku', 'N/A')} into queue. Queue size: {queue.qsize()}\")\n",
        "    logging.info(\"Producer: Finished loading all items.\")\n",
        "\n",
        "    # (Optional but Recommended) Add sentinel values (e.g., None) to signal completion\n",
        "    # If you have multiple consumers, add one sentinel per consumer.\n",
        "    # Add one sentinel per consumer\n",
        "    for _ in range(num_consumers):\n",
        "        await queue.put(None)\n",
        "    print(f\"Producer: Added {num_consumers} sentinel(s) to queue.\")\n",
        "\n",
        "\n",
        "async def process_items_from_queue(queue: asyncio.Queue, worker_id: int):\n",
        "    \"\"\"\n",
        "    Continuously gets items from the queue and processes them until a sentinel is received.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Consumer {worker_id}: Started...\")\n",
        "    while True:\n",
        "        item = await queue.get()\n",
        "        if item is None:\n",
        "            # Sentinel received, signal task completion\n",
        "            logging.info(f\"Consumer {worker_id}: Sentinel received. Exiting.\")\n",
        "            queue.task_done()\n",
        "            # Put the sentinel back if there are other consumers (not needed here as we only add one)\n",
        "            # await queue.put(None)\n",
        "            break # Exit the loop\n",
        "\n",
        "        # --- Process the item ---\n",
        "        new_brand = await async_generate_text(item.get('prompt'))\n",
        "        new_brand = new_brand.replace(\"'\",\"''\")\n",
        "        old_brand = item.get('brand')\n",
        "        old_brand = old_brand.replace(\"'\",\"''\")\n",
        "        sql = f\"\"\"UPDATE brand_mapping SET new_brand = '{new_brand}' WHERE brand = '{old_brand}';\"\"\"\n",
        "        print(sql)\n",
        "        await run_query(ecom_db_pool, sql)\n",
        "        logging.info(f\"Consumer {worker_id}: Finished processing brand: {item.get('brand')}. New brand: {new_brand}\")\n",
        "        # --- End processing ---\n",
        "\n",
        "        queue.task_done() # Signal that this item processing is complete\n",
        "\n",
        "\n",
        "async def generate_product_descriptions_concurrently(products_df):\n",
        "    # Create the queue. You can optionally set a maxsize.\n",
        "    # If maxsize is reached, the producer's `await queue.put(item)` will pause\n",
        "    # until a consumer calls `queue.get()`, providing backpressure.\n",
        "    work_queue = asyncio.Queue(maxsize=100)\n",
        "    num_consumers = 10\n",
        "\n",
        "    # Create tasks for the producer and consumer(s)\n",
        "    producer_task = asyncio.create_task(load_queue_from_dataframe(products_df, work_queue, num_consumers))\n",
        "\n",
        "    # Create one or more consumer tasks\n",
        "    consumer_tasks = []\n",
        "    for i in range(num_consumers):\n",
        "        consumer_tasks.append(\n",
        "            asyncio.create_task(process_items_from_queue(work_queue, i + 1))\n",
        "        )\n",
        "\n",
        "    # Wait for the producer to finish loading (optional, but good to ensure all items are queued)\n",
        "    await producer_task\n",
        "\n",
        "    # Wait for all consumers to finish processing all items\n",
        "    # This relies on consumers calling queue.task_done() for each item + the sentinel\n",
        "    await work_queue.join() # Wait until the queue is fully processed\n",
        "    logging.info(\"Main: Queue has been fully processed.\")\n",
        "\n",
        "\n",
        "await generate_product_descriptions_concurrently(brand_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ky5flD23snL-",
      "metadata": {
        "id": "Ky5flD23snL-"
      },
      "source": [
        "#### Remove New Lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t3v5y5sMsnGS",
      "metadata": {
        "id": "t3v5y5sMsnGS"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"\n",
        "UPDATE brand_mapping\n",
        "SET new_brand = REPLACE(new_brand, '\\n', '')\n",
        "\"\"\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A-9dMTz5r7oM",
      "metadata": {
        "id": "A-9dMTz5r7oM"
      },
      "source": [
        "#### Update Products Table with New Brands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Okyn9Q9kr7cT",
      "metadata": {
        "id": "Okyn9Q9kr7cT"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"\n",
        "UPDATE products p\n",
        "SET brand = SUBSTRING(b.new_brand, 0, 254),\n",
        "    name = SUBSTRING(REPLACE(name, '!!BRAND!!', b.new_brand), 0, 254)\n",
        "FROM brand_mapping b\n",
        "WHERE p.brand = b.brand;\n",
        "\"\"\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nB5ZzxzjyvME",
      "metadata": {
        "id": "nB5ZzxzjyvME"
      },
      "source": [
        "#### Update Inventory Items Table with New Brands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VaPUS32OyvE1",
      "metadata": {
        "id": "VaPUS32OyvE1"
      },
      "outputs": [],
      "source": [
        "\n",
        "sql = \"\"\"\n",
        "UPDATE inventory_items i\n",
        "SET product_name = p.name,\n",
        "    product_brand = p.brand\n",
        "FROM products p\n",
        "WHERE p.id = i.product_id\n",
        "\"\"\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yg7RJStl_xAf",
      "metadata": {
        "id": "Yg7RJStl_xAf"
      },
      "source": [
        "## Generate Product Descriptions with Gemini 2.0 Flash"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LvQ0V4pPl7Gu",
      "metadata": {
        "id": "LvQ0V4pPl7Gu"
      },
      "source": [
        "### Add product_description Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W10fm8rkl7AT",
      "metadata": {
        "id": "W10fm8rkl7AT"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"ALTER TABLE products ADD COLUMN product_description TEXT;\"\"\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CppKL_kmIl4b",
      "metadata": {
        "id": "CppKL_kmIl4b"
      },
      "source": [
        "### build_product_description_prompt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NqNCyM8qCKnw",
      "metadata": {
        "id": "NqNCyM8qCKnw"
      },
      "outputs": [],
      "source": [
        "def build_product_description_prompt(name, brand, category, department, retail_price, sku):\n",
        "  prompt = f\"\"\"**Persona & Context:**\n",
        "You are a skilled Copywriter for Cymbal Shops. Cymbal Shops is a specialty big box retailer offering a diverse, curated mix of trendy clothing, unique household knick-knacks, stylish furnishings, and interesting personal items. Our customers appreciate value, style, and finding items with personality. Our brand voice is:\n",
        "* Approachable & Friendly\n",
        "* Professional & Trustworthy\n",
        "* Slightly Quirky & Stylish\n",
        "* Focused on Value & Benefits\n",
        "\n",
        "**Task:**\n",
        "Write a compelling product description for the Cymbal Shops online product catalog page.\n",
        "\n",
        "**Output Requirements:**\n",
        "* **Length:** 50-75 words (approx. 3-5 sentences).\n",
        "* **Tone:** Match the Cymbal Shops brand voice described above.\n",
        "* **Goal:** Engage the customer and highlight the key benefits and appeal of the product. Translate features into benefits.\n",
        "* **Format:** A single paragraph of prose. Optionally, include 2-3 key feature bullet points after the main paragraph if features are distinct and numerous.\n",
        "* **Exclusions:** Do NOT mention the SKU or Retail Price within the written description.\n",
        "\n",
        "**Product Information to Use:**\n",
        "Product Name: {name}\n",
        "Brand: {brand}\n",
        "Category: {category}\n",
        "Department: {department}\n",
        "Retail Price: {retail_price}  (For context only, do not include in description)\n",
        "SKU: {sku} (For context only, do not include in description)\n",
        "\n",
        "**Now, write the description for the product detailed above.**\n",
        "\"\"\"\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TOhFCnVZIxJa",
      "metadata": {
        "id": "TOhFCnVZIxJa"
      },
      "source": [
        "### Get Products Without Descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5VATuVymAmaP",
      "metadata": {
        "id": "5VATuVymAmaP"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"SELECT id, name, brand, category, department, retail_price, sku FROM products WHERE product_description IS NULL;\"\"\"\n",
        "products_df = await run_query(ecom_db_pool, sql)\n",
        "products_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iH1B5MCsI1kj",
      "metadata": {
        "id": "iH1B5MCsI1kj"
      },
      "source": [
        "### Build Product Description Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WsGWY_laCrcL",
      "metadata": {
        "id": "WsGWY_laCrcL"
      },
      "outputs": [],
      "source": [
        "products_df['prompt'] = products_df.apply(\n",
        "    lambda row: build_product_description_prompt(\n",
        "        row['name'],\n",
        "        row['brand'],\n",
        "        row['category'],\n",
        "        row['department'],\n",
        "        row['retail_price'],\n",
        "        row['sku']\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "products_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C6JxblIAI4QD",
      "metadata": {
        "id": "C6JxblIAI4QD"
      },
      "source": [
        "### Generate Product Descriptions Asynchronously\n",
        "\n",
        "> NOTE: You may want to grab some coffee or tea. This step will take about 50 minutes to complete. You can adjust `work_queue` and `num_consumers` to balance processing speed vs throttling/quota limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mprQ8iSoT3Bd",
      "metadata": {
        "id": "mprQ8iSoT3Bd"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "async def load_queue_from_dataframe(df: pd.DataFrame, queue: asyncio.Queue, num_consumers: int):\n",
        "    \"\"\"\n",
        "    Iterates through DataFrame rows and puts them into the asyncio queue.\n",
        "\n",
        "    Args:\n",
        "        df: The Pandas DataFrame to process.\n",
        "        queue: The asyncio.Queue to put items into.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Producer: Starting to load {len(df)} items into the queue...\")\n",
        "    # Use itertuples for efficiency. index=False avoids adding the DataFrame index.\n",
        "    # name=None uses default namedtuple name 'Pandas'\n",
        "    for row_tuple in df.itertuples(index=False, name='Product'):\n",
        "        # Convert the named tuple to a dictionary - often easier for consumers\n",
        "        item = row_tuple._asdict()\n",
        "        await queue.put(item)\n",
        "        logging.info(f\"Producer: Put item {item.get('sku', 'N/A')} into queue. Queue size: {queue.qsize()}\")\n",
        "    logging.info(\"Producer: Finished loading all items.\")\n",
        "\n",
        "    # (Optional but Recommended) Add sentinel values (e.g., None) to signal completion\n",
        "    # If you have multiple consumers, add one sentinel per consumer.\n",
        "    # Add one sentinel per consumer\n",
        "    for _ in range(num_consumers):\n",
        "        await queue.put(None)\n",
        "    print(f\"Producer: Added {num_consumers} sentinel(s) to queue.\")\n",
        "\n",
        "\n",
        "async def process_items_from_queue(queue: asyncio.Queue, worker_id: int):\n",
        "    \"\"\"\n",
        "    Continuously gets items from the queue and processes them until a sentinel is received.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Consumer {worker_id}: Started...\")\n",
        "    while True:\n",
        "        item = await queue.get()\n",
        "        if item is None:\n",
        "            # Sentinel received, signal task completion\n",
        "            logging.info(f\"Consumer {worker_id}: Sentinel received. Exiting.\")\n",
        "            queue.task_done()\n",
        "            # Put the sentinel back if there are other consumers (not needed here as we only add one)\n",
        "            # await queue.put(None)\n",
        "            break # Exit the loop\n",
        "\n",
        "        # --- Process the item ---\n",
        "        product_description = await async_generate_text(item.get('prompt'))\n",
        "        product_description = product_description.replace(\"'\",\"''\")\n",
        "        sql = f\"UPDATE products SET product_description = '{product_description}' WHERE id = {item.get('id')};\"\n",
        "        await run_query(ecom_db_pool, sql)\n",
        "        logging.info(f\"Consumer {worker_id}: Finished processing ID: {item.get('id')}, SKU: {item.get('sku')}\")\n",
        "        # --- End processing ---\n",
        "\n",
        "        queue.task_done() # Signal that this item processing is complete\n",
        "\n",
        "\n",
        "async def generate_product_descriptions_concurrently(products_df):\n",
        "    # Create the queue. You can optionally set a maxsize.\n",
        "    # If maxsize is reached, the producer's `await queue.put(item)` will pause\n",
        "    # until a consumer calls `queue.get()`, providing backpressure.\n",
        "    work_queue = asyncio.Queue(maxsize=100)\n",
        "    num_consumers = 10\n",
        "\n",
        "    # Create tasks for the producer and consumer(s)\n",
        "    producer_task = asyncio.create_task(load_queue_from_dataframe(products_df, work_queue, num_consumers))\n",
        "\n",
        "    # Create one or more consumer tasks\n",
        "    consumer_tasks = []\n",
        "    for i in range(num_consumers):\n",
        "        consumer_tasks.append(\n",
        "            asyncio.create_task(process_items_from_queue(work_queue, i + 1))\n",
        "        )\n",
        "\n",
        "    # Wait for the producer to finish loading (optional, but good to ensure all items are queued)\n",
        "    await producer_task\n",
        "\n",
        "    # Wait for all consumers to finish processing all items\n",
        "    # This relies on consumers calling queue.task_done() for each item + the sentinel\n",
        "    await work_queue.join() # Wait until the queue is fully processed\n",
        "    logging.info(\"Main: Queue has been fully processed.\")\n",
        "\n",
        "\n",
        "await generate_product_descriptions_concurrently(products_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uRN1QLpmIG10",
      "metadata": {
        "id": "uRN1QLpmIG10"
      },
      "source": [
        "### View Product Descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QuMm_dGpIGvF",
      "metadata": {
        "id": "QuMm_dGpIGvF"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT id, name, brand, category, department, sku, product_description FROM products LIMIT 5;\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YokkuJG8zRto",
      "metadata": {
        "id": "YokkuJG8zRto"
      },
      "source": [
        "## Generate Product Images with Imagen 3\n",
        "\n",
        "> IMPORTANT: This section uses the `imagen-3.0-fast-generate-001` model to generate product images for a dataset containing 29,120 products. As of the time of publishing this notebook, the model costs $0.02 per image. There is a default limit of 1000 images set below to prevent inadvertently running a cost job, but you can adjust that limit up or down as desired.\n",
        "\n",
        "> NOTE: If you would like to generate pictures of people, ensure your project is allow-listed first. You can request to be allow-listed using [this form](https://docs.google.com/forms/d/e/1FAIpQLSduBp9w84qgim6vLriQ9p7sdz62bMJaL-nNmIVoyiOwd84SMw/viewform)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21jjSkUyQU6e",
      "metadata": {
        "id": "21jjSkUyQU6e"
      },
      "source": [
        "### Create Coming Soon Image\n",
        "\n",
        "You can run this cell repeatedly until you're happy with the placeholder image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BVvG8oasQO4W",
      "metadata": {
        "id": "BVvG8oasQO4W"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "coming_soon_uri = generate_image('Professional image with gray background and bold white lettering that says \"Coming Soon\"', 'coming_soon', 1)\n",
        "print(f\"gsutil uri: {coming_soon_uri}\")\n",
        "\n",
        "display(Image(url=f\"https://storage.cloud.google.com/{image_bucket}/product-images/coming_soon.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M8-pS0GP0F-Y",
      "metadata": {
        "id": "M8-pS0GP0F-Y"
      },
      "source": [
        "### Add product_image_uri Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a24g1CWz0F-Z",
      "metadata": {
        "id": "a24g1CWz0F-Z"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"ALTER TABLE products ADD COLUMN product_image_uri TEXT;\"\"\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oSDA-wtsxu9j",
      "metadata": {
        "id": "oSDA-wtsxu9j"
      },
      "source": [
        "### Use Placeholder Image for Sentitive Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LM1JNw33xu1T",
      "metadata": {
        "id": "LM1JNw33xu1T"
      },
      "outputs": [],
      "source": [
        "#coming_soon_uri = 'gs://stylesearch-masked/product-images/coming_soon.png'\n",
        "\n",
        "sql = f\"\"\"\n",
        "UPDATE products\n",
        "SET product_image_uri = '{coming_soon_uri}'\n",
        "WHERE category IN ('Intimates','Underwear')\n",
        "\"\"\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mWgsGid-0wCx",
      "metadata": {
        "id": "mWgsGid-0wCx"
      },
      "outputs": [],
      "source": [
        "sql = f\"\"\"\n",
        "SELECT COUNT(*) FROM products\n",
        "WHERE product_image_uri IS NULL\n",
        "\"\"\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hY0i0CgK0F-a",
      "metadata": {
        "id": "hY0i0CgK0F-a"
      },
      "source": [
        "### Define Product Image Prompt Builder Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AT7J3FKp0F-a",
      "metadata": {
        "id": "AT7J3FKp0F-a"
      },
      "outputs": [],
      "source": [
        "def build_product_image_prompt(name, brand, category, department):\n",
        "  # Remove breaking quotes\n",
        "  name = name.replace(\"'\", \"\")\n",
        "\n",
        "  # Remove terms that trigger image generation failures\n",
        "  remove = [\n",
        "      brand,\n",
        "      '-',\n",
        "      'Boys',\n",
        "      'Boy',\n",
        "      'Girls',\n",
        "      'Girl',\n",
        "      'Juniors',\n",
        "      'Junior',\n",
        "      '.',\n",
        "      '&',\n",
        "      '\\n',\n",
        "  ]\n",
        "  for r in remove:\n",
        "      name = name.lower().replace(r.lower(), '')\n",
        "  name = name.strip()\n",
        "  prompt = f\"Product image (with NO logos): {department} {name} {category} {brand}\"\n",
        "  prompt = prompt[0:1200] # Shorten long prompts to fit within token limit\n",
        "  if not prompt:\n",
        "    prompt = 'Coming Soon'\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "czYHt6MW0F-b",
      "metadata": {
        "id": "czYHt6MW0F-b"
      },
      "source": [
        "### Get Products Without Product Images\n",
        "\n",
        "> IMPORTANT: This is the cell that builds the dataset that will be used to generate photos. You can adjust the limit up or down as desired to control the cost of the image generation job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vvwsyAEe0F-b",
      "metadata": {
        "id": "vvwsyAEe0F-b"
      },
      "outputs": [],
      "source": [
        "# Set the number of products to generate images for here\n",
        "image_limit = 30000\n",
        "\n",
        "# Get products without images\n",
        "sql = f\"\"\"SELECT id,\n",
        "    name,\n",
        "    brand,\n",
        "    category,\n",
        "    department,\n",
        "    retail_price,\n",
        "    sku,\n",
        "    product_description\n",
        "  FROM products\n",
        "  WHERE product_image_uri IS NULL\n",
        "  AND name IS NOT NULL\n",
        "  AND brand IS NOT NULL\n",
        "  AND name != 'Discontinued'\n",
        "  LIMIT {image_limit};\"\"\"\n",
        "products_df = await run_query(ecom_db_pool, sql)\n",
        "products_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "naUYCfOz0F-b",
      "metadata": {
        "id": "naUYCfOz0F-b"
      },
      "source": [
        "### Build Image Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2sWA9lS0F-b",
      "metadata": {
        "id": "e2sWA9lS0F-b"
      },
      "outputs": [],
      "source": [
        "products_df['image_prompt'] = products_df.apply(\n",
        "    lambda row: build_product_image_prompt(\n",
        "        row['name'],\n",
        "        row['brand'],\n",
        "        row['category'],\n",
        "        row['department'],\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "products_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V5qcPKFgZLQr",
      "metadata": {
        "id": "V5qcPKFgZLQr"
      },
      "source": [
        "### Generate Images Asynchronously\n",
        "\n",
        "This step will take 4-5 hours to generate images for all 29,120 products in the Cymbal Shops product catalog, running 150 async requests at a time. You can adjust the number of images to generate in the cells above (see comments). You can also request a quota increase to allow more concurrent invocations, in which case you can increase the `num_consumers` variable below.\n",
        "\n",
        "> NOTE: Errors are expected in this step due to ambiguous product names and content filter false positives. Failures will be marked in the `product_image_uri` column in the database. Early testing resulted in an ~80% success rate. You can tweak the prompt and retry for failed items if desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UW0g3OSEzRlh",
      "metadata": {
        "id": "UW0g3OSEzRlh"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "\n",
        "async def load_queue_from_dataframe(df: pd.DataFrame, queue: asyncio.Queue, num_consumers: int):\n",
        "    \"\"\"\n",
        "    Iterates through DataFrame rows and puts them into the asyncio queue.\n",
        "\n",
        "    Args:\n",
        "        df: The Pandas DataFrame to process.\n",
        "        queue: The asyncio.Queue to put items into.\n",
        "    \"\"\"\n",
        "    print(f\"Producer: Starting to load {len(df)} items into the queue...\")\n",
        "    # Use itertuples for efficiency. index=False avoids adding the DataFrame index.\n",
        "    # name=None uses default namedtuple name 'Pandas'\n",
        "    for row_tuple in df.itertuples(index=False, name='Product'):\n",
        "        # Convert the named tuple to a dictionary - often easier for consumers\n",
        "        item = row_tuple._asdict()\n",
        "        await queue.put(item)\n",
        "        logging.info(f\"Producer: Put item {item.get('sku', 'N/A')} into queue. Queue size: {queue.qsize()}\")\n",
        "    logging.info(\"Producer: Finished loading all items.\")\n",
        "\n",
        "    # Add one sentinel value (e.g., None) per consumer to signal completion\n",
        "    for _ in range(num_consumers):\n",
        "        await queue.put(None)\n",
        "    print(f\"Producer: Added {num_consumers} sentinel(s) to queue.\")\n",
        "\n",
        "\n",
        "async def process_items_from_queue(queue: asyncio.Queue, worker_id: int):\n",
        "    \"\"\"\n",
        "    Continuously gets items from the queue and processes them until a sentinel is received.\n",
        "    Propagates exceptions if processing fails.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Consumer {worker_id}: Started...\")\n",
        "    while True:\n",
        "        item = await queue.get()\n",
        "\n",
        "        # --- Check for Sentinel ---\n",
        "        if item is None:\n",
        "            print(f\"Consumer {worker_id}: Sentinel received. Exiting.\")\n",
        "            queue.task_done() # Mark sentinel processing as done\n",
        "            break # Exit the loop\n",
        "\n",
        "        # --- Process the item ---\n",
        "\n",
        "        try:\n",
        "            log_prefix = f\"Consumer {worker_id}: Item ID {item.get('id', 'N/A')}:\"\n",
        "            logging.info(f\"{log_prefix} Starting processing.\")\n",
        "\n",
        "            # Get current running loop\n",
        "            loop = asyncio.get_running_loop()\n",
        "\n",
        "            # Run the blocking image generation function\n",
        "            # Ensure generate_image raises exceptions on failure or returns None clearly\n",
        "            image_uri = await loop.run_in_executor(\n",
        "                None, # Use default executor (ThreadPoolExecutor)\n",
        "\n",
        "                # --- Define the blocking function to run ---\n",
        "                generate_image,\n",
        "\n",
        "                # --- Add function parameters here ---\n",
        "                item.get('image_prompt'), # First argument for generate_image\n",
        "                item.get('sku'),          # Second argument for generate_image, etc\n",
        "                item.get('id'),\n",
        "            )\n",
        "\n",
        "            # Handle image generation failure (if it returns None instead of raising)\n",
        "            if image_uri is None:\n",
        "                # Log warning and continue (skip DB update for this item)\n",
        "                logging.warning(f\"{log_prefix} Image generation failed or returned None.\")\n",
        "                sql = f\"UPDATE products SET product_image_uri = 'FAILED - Prompt: {item.get('image_prompt')}' WHERE id = {item.get('id')};\"\n",
        "                await run_query(ecom_db_pool, sql)\n",
        "            else:\n",
        "                logging.info(f\"{log_prefix} Image generated: '{image_uri}'\")\n",
        "                # Run the database update\n",
        "                sql = f\"UPDATE products SET product_image_uri = '{image_uri}' WHERE id = {item.get('id')};\"\n",
        "                await run_query(ecom_db_pool, sql)\n",
        "                #logging.info(f\"{log_prefix} Database updated successfully.\")\n",
        "\n",
        "            # --- Processing successful for this item ---\n",
        "            queue.task_done() # Signal completion ONLY on success\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log the exception WITH traceback\n",
        "            logging.error(f\"Consumer {worker_id}: Unhandled exception processing item ID {item.get('id', 'N/A')}: {e}\", exc_info=True)\n",
        "            # Log error without raising so that remaining items can be processed.\n",
        "            logging.warning(e)\n",
        "\n",
        "\n",
        "async def process_dataframe_concurrently(products_df):\n",
        "    # Create the queue with maxsize. If maxsize is reached, the producer's\n",
        "    # `await queue.put(item)` will pause until a consumer calls `queue.get()`, providing backpressure.\n",
        "\n",
        "    # --- Set queue max size and number of consumers/workers here ---\n",
        "    work_queue = asyncio.Queue(maxsize=100)\n",
        "    num_consumers = 10\n",
        "    all_tasks = []\n",
        "\n",
        "    # --- Create Tasks ---\n",
        "    print(\"Main: Creating producer task...\")\n",
        "    producer_task = asyncio.create_task(\n",
        "        load_queue_from_dataframe(products_df, work_queue, num_consumers),\n",
        "        name=\"Producer\"\n",
        "    )\n",
        "    all_tasks.append(producer_task)\n",
        "\n",
        "    print(f\"Main: Creating {num_consumers} consumer tasks...\")\n",
        "    for i in range(num_consumers):\n",
        "        consumer_task = asyncio.create_task(\n",
        "            process_items_from_queue(work_queue, i + 1),\n",
        "            name=f\"Consumer-{i+1}\"\n",
        "        )\n",
        "        all_tasks.append(consumer_task)\n",
        "\n",
        "    # Wait for the producer to finish loading (optional, but good to ensure all items are queued)\n",
        "    await producer_task\n",
        "\n",
        "    # --- Run Tasks and Handle Completion/Failure ---\n",
        "    done, pending = [], []\n",
        "    try:\n",
        "        # Wait for all tasks to complete. gather will raise the *first* exception\n",
        "        # encountered in any of the tasks.\n",
        "        print(\"Main: Waiting for tasks to complete...\")\n",
        "        # Use asyncio.wait instead of gather to have more control over pending tasks on error\n",
        "        done, pending = await asyncio.wait(all_tasks, return_when=asyncio.FIRST_COMPLETED)\n",
        "\n",
        "        # Check if any completed tasks raised an exception\n",
        "        for task in done:\n",
        "            if task.exception():\n",
        "                raise task.exception() # Raise the exception from the failed task\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Main: An error occurred in a task: {e}\", exc_info=True)\n",
        "        print(\"Main: Attempting to cancel pending tasks...\")\n",
        "        for task in pending: # Cancel tasks found in the pending set from asyncio.wait\n",
        "             task.cancel()\n",
        "\n",
        "        # Give cancelled tasks a moment to process the cancellation\n",
        "        # and gather any CancelledError exceptions (optional but cleaner)\n",
        "        if pending:\n",
        "             await asyncio.wait(pending, timeout=1.0) # Wait briefly\n",
        "\n",
        "        # Important: Re-raise the original exception to stop the program execution\n",
        "        # Or handle it appropriately (e.g., sys.exit(1))\n",
        "        raise e # Propagate the error out\n",
        "\n",
        "    finally:\n",
        "        # Ensure all tasks are truly finished one way or another (optional cleanup)\n",
        "        remaining_tasks = [t for t in all_tasks if not t.done()]\n",
        "        if remaining_tasks:\n",
        "             print(\"Main: Waiting for final cleanup of any remaining tasks...\")\n",
        "             await asyncio.wait(remaining_tasks, timeout=1.0) # Brief wait\n",
        "\n",
        "    # If execution reaches here, it means all tasks finished without unhandled exceptions propagating\n",
        "    print(\"Main: Process finished.\")\n",
        "\n",
        "# This is a very verbose process. Changing the logging level to WARNING\n",
        "logging.basicConfig(level=logging.WARNING, stream=sys.stdout, format='%(asctime)s[%(levelname)5s][%(name)14s] - %(message)s',  datefmt='%H:%M:%S', force=True)\n",
        "\n",
        "# Kick off parallel image creation\n",
        "await process_dataframe_concurrently(products_df)\n",
        "\n",
        "# Switch logging back to INFO\n",
        "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='%(asctime)s[%(levelname)5s][%(name)14s] - %(message)s',  datefmt='%H:%M:%S', force=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BtthMdaYw0Un",
      "metadata": {
        "id": "BtthMdaYw0Un"
      },
      "source": [
        "### View Image Generation Success Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ExFoPEw0OP",
      "metadata": {
        "id": "d3ExFoPEw0OP"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"WITH a AS (\n",
        "  SELECT\n",
        "    (SELECT COUNT(*)  FROM products WHERE product_image_uri IS NOT NULL) AS processed,\n",
        "    (SELECT COUNT(*) FROM products WHERE product_image_uri LIKE 'FAILED%') AS failed\n",
        ") SELECT processed,\n",
        "  processed - failed AS successful,\n",
        "  failed,\n",
        "  1-(failed/processed::FLOAT) AS success_rate\n",
        "  FROM a;\"\"\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dqpz_fpYQPAA",
      "metadata": {
        "id": "dqpz_fpYQPAA"
      },
      "source": [
        "## Clean Up Image Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cVqvGETWQXji",
      "metadata": {
        "id": "cVqvGETWQXji"
      },
      "source": [
        "### Set Image to \"Coming Soon\" for NULLs and Incorrectly Generated Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RBvaV2H5QXCw",
      "metadata": {
        "id": "RBvaV2H5QXCw"
      },
      "outputs": [],
      "source": [
        "coming_soon_uri = f\"gs://{image_bucket}/product-images/coming_soon.png\"\n",
        "\n",
        "sql = \"SELECT COUNT(*) FROM products WHERE product_image_uri IS NULL OR product_image_uri LIKE 'FAILED%';\"\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "print(result)\n",
        "\n",
        "sql = f\"\"\"\n",
        "  UPDATE products\n",
        "  SET product_image_uri = '{coming_soon_uri}'\n",
        "  WHERE product_image_uri IS NULL\n",
        "  OR product_image_uri LIKE 'FAILED%';\"\"\"\n",
        "\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4RuHezJYI82Q",
      "metadata": {
        "id": "4RuHezJYI82Q"
      },
      "source": [
        "## Generate Multimodal Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RsOZc9fvJCFd",
      "metadata": {
        "id": "RsOZc9fvJCFd"
      },
      "source": [
        "### Add Embedding Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T8P_sFW6I-2q",
      "metadata": {
        "id": "T8P_sFW6I-2q"
      },
      "outputs": [],
      "source": [
        "sql_array = []\n",
        "sql_array.append(\"ALTER TABLE products ADD COLUMN product_description_embedding VECTOR(1408);\")\n",
        "sql_array.append(\"ALTER TABLE products ADD COLUMN product_description_embedding_model TEXT;\")\n",
        "sql_array.append(\"ALTER TABLE products ADD COLUMN product_image_embedding VECTOR(1408);\")\n",
        "sql_array.append(\"ALTER TABLE products ADD COLUMN product_image_embedding_model TEXT;\")\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d_1-6TsySxyL",
      "metadata": {
        "id": "d_1-6TsySxyL"
      },
      "source": [
        "### Get Products to Embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HbC_Xo95R45i",
      "metadata": {
        "id": "HbC_Xo95R45i"
      },
      "outputs": [],
      "source": [
        "sql = f\"\"\"SELECT id,\n",
        "            name,\n",
        "            brand,\n",
        "            category,\n",
        "            department,\n",
        "            retail_price,\n",
        "            sku,\n",
        "            product_description,\n",
        "            product_image_uri\n",
        "         FROM products\n",
        "         WHERE product_description_embedding IS NULL\n",
        "         AND product_image_uri != 'gs://{image_bucket}/product-images/coming_soon.png'\n",
        "         LIMIT 30000;\"\"\"\n",
        "products_df = await run_query(ecom_db_pool, sql)\n",
        "products_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HIFDvc9GLG_z",
      "metadata": {
        "id": "HIFDvc9GLG_z"
      },
      "source": [
        "### Generate Multimodal Embeddings Asynchronously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h327gokqLFCJ",
      "metadata": {
        "id": "h327gokqLFCJ"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import vertexai\n",
        "from vertexai.vision_models import Image, MultiModalEmbeddingModel\n",
        "\n",
        "\n",
        "async def load_queue_from_dataframe(df: pd.DataFrame, queue: asyncio.Queue, num_consumers: int):\n",
        "    \"\"\"\n",
        "    Iterates through DataFrame rows and puts them into the asyncio queue.\n",
        "\n",
        "    Args:\n",
        "        df: The Pandas DataFrame to process.\n",
        "        queue: The asyncio.Queue to put items into.\n",
        "    \"\"\"\n",
        "    print(f\"Producer: Starting to load {len(df)} items into the queue...\")\n",
        "    # Use itertuples for efficiency. index=False avoids adding the DataFrame index.\n",
        "    # name=None uses default namedtuple name 'Pandas'\n",
        "    for row_tuple in df.itertuples(index=False, name='Product'):\n",
        "        # Convert the named tuple to a dictionary - often easier for consumers\n",
        "        item = row_tuple._asdict()\n",
        "        await queue.put(item)\n",
        "        logging.info(f\"Producer: Put item {item.get('sku', 'N/A')} into queue. Queue size: {queue.qsize()}\")\n",
        "    logging.info(\"Producer: Finished loading all items.\")\n",
        "\n",
        "    # Add one sentinel value (e.g., None) per consumer to signal completion\n",
        "    for _ in range(num_consumers):\n",
        "        await queue.put(None)\n",
        "    print(f\"Producer: Added {num_consumers} sentinel(s) to queue.\")\n",
        "\n",
        "\n",
        "async def process_items_from_queue(queue: asyncio.Queue, worker_id: int):\n",
        "    \"\"\"\n",
        "    Continuously gets items from the queue and processes them until a sentinel is received.\n",
        "    Propagates exceptions if processing fails.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Consumer {worker_id}: Started...\")\n",
        "    while True:\n",
        "        item = await queue.get()\n",
        "\n",
        "        # --- Check for Sentinel ---\n",
        "        if item is None:\n",
        "            print(f\"Consumer {worker_id}: Sentinel received. Exiting.\")\n",
        "            queue.task_done() # Mark sentinel processing as done\n",
        "            break # Exit the loop\n",
        "\n",
        "        # --- Process the item ---\n",
        "\n",
        "        try:\n",
        "            log_prefix = f\"Consumer {worker_id}: Item ID {item.get('id', 'N/A')}:\"\n",
        "            logging.info(f\"{log_prefix} Starting processing.\")\n",
        "\n",
        "            # Get current running loop\n",
        "            loop = asyncio.get_running_loop()\n",
        "\n",
        "            # Define variables\n",
        "            prompt = f\"{item.get('name')} {item.get('product_description')}\"\n",
        "            prompt = prompt.replace(\"'\",\"\")\n",
        "            uri = item.get('product_image_uri')\n",
        "            if uri:\n",
        "                if uri.startswith('FAILED'):\n",
        "                   uri = None\n",
        "\n",
        "            # Run the blocking embedding generation function\n",
        "            result = await loop.run_in_executor(\n",
        "                None, # Use default executor (ThreadPoolExecutor)\n",
        "\n",
        "                # --- Define the io blocking function to run ---\n",
        "                generate_multimodal_embeddings,\n",
        "\n",
        "                # --- Add function parameters here ---\n",
        "                uri,      # First argument for generate_multimodal_embeddings\n",
        "                prompt,   # Second argument for generate_multimodal_embeddings, etc\n",
        "            )\n",
        "\n",
        "            # Handle embedding generation failure (if it returns None instead of raising)\n",
        "            if result is None:\n",
        "                # Log warning and continue (skip DB update for this item)\n",
        "                logging.warning(f\"{log_prefix} Embedding generation failed or returned None.\")\n",
        "            else:\n",
        "                # Build the UPDATE statement\n",
        "                sql = f\"\"\"UPDATE products\n",
        "                    SET product_description_embedding = '{result.text_embedding}',\n",
        "                        product_description_embedding_model = 'multimodalembedding@001'\"\"\"\n",
        "\n",
        "                if result.image_embedding:\n",
        "                    sql = sql + f\"\"\",\n",
        "                        product_image_embedding = '{result.image_embedding}',\n",
        "                        product_image_embedding_model = 'multimodalembedding@001'\"\"\"\n",
        "                else:\n",
        "                    logging.info(\"No image embedding received in payload.\")\n",
        "\n",
        "                # Run the database UPDATE\n",
        "                sql = sql + f\" WHERE id = {item.get('id')};\"\n",
        "                await run_query(ecom_db_pool, sql)\n",
        "\n",
        "            # --- Processing successful for this item ---\n",
        "            queue.task_done() # Signal completion ONLY on success\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log the exception WITH traceback\n",
        "            logging.error(f\"Consumer {worker_id}: Unhandled exception processing item ID {item.get('id', 'N/A')}: {e}\", exc_info=True)\n",
        "            # Log error without raising so that remaining items can be processed.\n",
        "            logging.warning(e)\n",
        "\n",
        "\n",
        "async def process_dataframe_concurrently(products_df):\n",
        "    # Create the queue with maxsize. If maxsize is reached, the producer's\n",
        "    # `await queue.put(item)` will pause until a consumer calls `queue.get()`, providing backpressure.\n",
        "\n",
        "    # --- Set queue max size and number of consumers/workers here ---\n",
        "    work_queue = asyncio.Queue(maxsize=800)\n",
        "    num_consumers = 100\n",
        "    all_tasks = []\n",
        "\n",
        "    # --- Create Tasks ---\n",
        "    print(\"Main: Creating producer task...\")\n",
        "    producer_task = asyncio.create_task(\n",
        "        load_queue_from_dataframe(products_df, work_queue, num_consumers),\n",
        "        name=\"Producer\"\n",
        "    )\n",
        "    all_tasks.append(producer_task)\n",
        "\n",
        "    print(f\"Main: Creating {num_consumers} consumer tasks...\")\n",
        "    for i in range(num_consumers):\n",
        "        consumer_task = asyncio.create_task(\n",
        "            process_items_from_queue(work_queue, i + 1),\n",
        "            name=f\"Consumer-{i+1}\"\n",
        "        )\n",
        "        all_tasks.append(consumer_task)\n",
        "\n",
        "    # Wait for the producer to finish loading (optional, but good to ensure all items are queued)\n",
        "    await producer_task\n",
        "\n",
        "    # --- Run Tasks and Handle Completion/Failure ---\n",
        "    done, pending = [], []\n",
        "    try:\n",
        "        # Wait for all tasks to complete. gather will raise the *first* exception\n",
        "        # encountered in any of the tasks.\n",
        "        print(\"Main: Waiting for tasks to complete...\")\n",
        "        # Use asyncio.wait instead of gather to have more control over pending tasks on error\n",
        "        done, pending = await asyncio.wait(all_tasks, return_when=asyncio.FIRST_COMPLETED)\n",
        "\n",
        "        # Check if any completed tasks raised an exception\n",
        "        for task in done:\n",
        "            if task.exception():\n",
        "                raise task.exception() # Raise the exception from the failed task\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Main: An error occurred in a task: {e}\", exc_info=True)\n",
        "        print(\"Main: Attempting to cancel pending tasks...\")\n",
        "        for task in pending: # Cancel tasks found in the pending set from asyncio.wait\n",
        "             task.cancel()\n",
        "\n",
        "        # Give cancelled tasks a moment to process the cancellation\n",
        "        # and gather any CancelledError exceptions (optional but cleaner)\n",
        "        if pending:\n",
        "             await asyncio.wait(pending, timeout=1.0) # Wait briefly\n",
        "\n",
        "        # Important: Re-raise the original exception to stop the program execution\n",
        "        # Or handle it appropriately (e.g., sys.exit(1))\n",
        "        raise e # Propagate the error out\n",
        "\n",
        "    finally:\n",
        "        # Ensure all tasks are truly finished one way or another (optional cleanup)\n",
        "        remaining_tasks = [t for t in all_tasks if not t.done()]\n",
        "        if remaining_tasks:\n",
        "             print(\"Main: Waiting for final cleanup of any remaining tasks...\")\n",
        "             await asyncio.wait(remaining_tasks, timeout=1.0) # Brief wait\n",
        "\n",
        "    # If execution reaches here, it means all tasks finished without unhandled exceptions propagating\n",
        "    print(\"Main: Process finished.\")\n",
        "\n",
        "# This is a very verbose process. Changing the logging level to WARNING\n",
        "logging.basicConfig(level=logging.WARNING, stream=sys.stdout, format='%(asctime)s[%(levelname)5s][%(name)14s] - %(message)s',  datefmt='%H:%M:%S', force=True)\n",
        "\n",
        "# Kick off parallel image creation\n",
        "await process_dataframe_concurrently(products_df)\n",
        "\n",
        "# Switch logging back to INFO\n",
        "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='%(asctime)s[%(levelname)5s][%(name)14s] - %(message)s',  datefmt='%H:%M:%S', force=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LiQwk7CNgt4A",
      "metadata": {
        "id": "LiQwk7CNgt4A"
      },
      "source": [
        "### Verify Embedding Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YHovEs-gPFUz",
      "metadata": {
        "id": "YHovEs-gPFUz"
      },
      "outputs": [],
      "source": [
        "sql = f\"\"\"SELECT 'product_description' AS column_name, (SELECT COUNT(*) FROM products WHERE product_description IS NULL) AS null_count\n",
        "UNION ALL\n",
        "SELECT 'product_description_embedding', (SELECT COUNT(*) FROM products WHERE product_description_embedding IS NULL AND product_image_uri != 'gs://{image_bucket}/product-images/coming_soon.png')\n",
        "UNION ALL\n",
        "SELECT 'product_image_uri', (SELECT COUNT(*) FROM products WHERE product_image_uri IS NULL OR product_image_uri LIKE 'FAILED%')\n",
        "UNION ALL\n",
        "SELECT 'product_image_embedding', (SELECT COUNT(*) FROM products WHERE product_image_embedding IS NULL AND product_image_uri != 'gs://{image_bucket}/product-images/coming_soon.png');\"\"\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZKL9_gvNiRks",
      "metadata": {
        "id": "ZKL9_gvNiRks"
      },
      "source": [
        "## Generate Semantic Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9_nk63sryT2s",
      "metadata": {
        "id": "9_nk63sryT2s"
      },
      "source": [
        "### Replace Embedding Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s1Y29QZQyTmU",
      "metadata": {
        "id": "s1Y29QZQyTmU"
      },
      "outputs": [],
      "source": [
        "sql_array = []\n",
        "sql_array.append(\"ALTER TABLE products DROP COLUMN embedding;\")\n",
        "sql_array.append(\"ALTER TABLE products DROP COLUMN embedding_model_version;\")\n",
        "sql_array.append(\"ALTER TABLE products ADD COLUMN product_embedding VECTOR(3072);\")\n",
        "sql_array.append(\"ALTER TABLE products ADD COLUMN product_embedding_model TEXT;\")\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YNVAQGD20xrM",
      "metadata": {
        "id": "YNVAQGD20xrM"
      },
      "source": [
        "### Embed Products Asynchronously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vcB4yxGm0xgy",
      "metadata": {
        "id": "vcB4yxGm0xgy"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import math\n",
        "\n",
        "# Update logging to level=WARN due to verbose output\n",
        "logging.basicConfig(level=logging.WARN, stream=sys.stdout, format='%(asctime)s[%(levelname)5s][%(name)14s] - %(message)s',  datefmt='%H:%M:%S', force=True)\n",
        "\n",
        "async def apply_async_func_to_dataframe(df, func, column_to_embed, new_column_name):\n",
        "    \"\"\"\n",
        "    Applies an asynchronous function to a specified column of a DataFrame\n",
        "    and stores the results in a new column.\n",
        "    \"\"\"\n",
        "    # Create a list of coroutine objects\n",
        "    tasks = [func(row[column_to_embed]) for index, row in df.iterrows()]\n",
        "\n",
        "    # Run all coroutines concurrently\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Assign the results back to the DataFrame\n",
        "    df[new_column_name] = results\n",
        "    return df\n",
        "\n",
        "async def main():\n",
        "    # Get count of rows to embed\n",
        "    sql = \"\"\"\n",
        "      SELECT COUNT(*)\n",
        "      FROM products\n",
        "      WHERE product_embedding IS NULL;\n",
        "      \"\"\"\n",
        "    row_count = await run_query(ecom_db_pool, sql, output_as_df=False)\n",
        "    rows_to_embed = row_count.fetchall()[0][0]\n",
        "    print(f\"Rows to embed: {rows_to_embed}\")\n",
        "\n",
        "    # Generate embeddings in batches\n",
        "    batch_size = 100\n",
        "    batch_count = math.ceil(rows_to_embed / batch_size)\n",
        "    current_batch = 1\n",
        "\n",
        "    while current_batch <= batch_count:\n",
        "      # Define the dataframe\n",
        "      sql = f\"\"\"\n",
        "        SELECT id, CONCAT('Name: ', name, ' \\\\nCategory: ', category, ' \\\\nBrand: ', brand, ' \\\\nDepartment: ', department) AS embed_string\n",
        "        FROM products\n",
        "        WHERE product_embedding IS NULL\n",
        "        LIMIT {batch_size};\n",
        "        \"\"\"\n",
        "      semantic_embedding_df = await run_query(ecom_db_pool, sql)\n",
        "\n",
        "      print(f\"Batch {current_batch} of {batch_count}: Running async embedding job...\")\n",
        "\n",
        "      # Apply the async function\n",
        "      semantic_embedding_df = await apply_async_func_to_dataframe(\n",
        "          semantic_embedding_df,\n",
        "          async_generate_embedding,\n",
        "          'embed_string',\n",
        "          'product_embedding'\n",
        "      )\n",
        "\n",
        "      # Update the database for the current batch\n",
        "      # Use bulk executemany for more efficient update\n",
        "      print(f\"Batch {current_batch} of {batch_count}: Updating database with embeddings...\")\n",
        "      data_to_update = [\n",
        "          {\"product_embedding\": str(row.product_embedding), \"id\": row.id}\n",
        "          for row in semantic_embedding_df.itertuples(index=False) # Use index=False since id is a column, not a df index\n",
        "      ]\n",
        "\n",
        "      sql = f\"\"\"\n",
        "      UPDATE products\n",
        "      SET product_embedding = :product_embedding,\n",
        "          product_embedding_model = 'gemini-embedding-001'\n",
        "      WHERE id = :id;\n",
        "      \"\"\"\n",
        "      await run_query(ecom_db_pool, sql, data_to_update)\n",
        "\n",
        "      current_batch = current_batch + 1\n",
        "\n",
        "# To run this, you need to execute the main async function\n",
        "if __name__ == \"__main__\":\n",
        "    # For Jupyter notebooks or environments where an event loop might already be running:\n",
        "    try:\n",
        "        loop = asyncio.get_running_loop()\n",
        "    except RuntimeError: # No running event loop\n",
        "        loop = None\n",
        "\n",
        "    if loop and loop.is_running():\n",
        "        # If a loop is already running, schedule the task\n",
        "        task = loop.create_task(main())\n",
        "        print(\"Scheduled main() function to run in existing event loop.\")\n",
        "        await task\n",
        "    else:\n",
        "        # If no loop is running, create and run a new one\n",
        "        task = asyncio.run(main())\n",
        "        print(\"Scheduled main() function to run in new event loop.\")\n",
        "        await task\n",
        "\n",
        "\n",
        "# Update logging back to level=INFO\n",
        "logging.basicConfig(level=logging.WARN, stream=sys.stdout, format='%(asctime)s[%(levelname)5s][%(name)14s] - %(message)s',  datefmt='%H:%M:%S', force=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7NuROwUgOxHK",
      "metadata": {
        "id": "7NuROwUgOxHK"
      },
      "source": [
        "### Ensure All Rows Have Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gaq4OxzoOw-d",
      "metadata": {
        "id": "gaq4OxzoOw-d"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT COUNT(*) FROM products WHERE product_embedding IS NULL\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "csvHXIIFhXkR",
      "metadata": {
        "id": "csvHXIIFhXkR"
      },
      "source": [
        "## Generate BM25 Sparse Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DXlu0x1nmmzB",
      "metadata": {
        "id": "DXlu0x1nmmzB"
      },
      "source": [
        "### Create or Instantiate Bucket for BM25 Index Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FdO09WicmmzJ",
      "metadata": {
        "id": "FdO09WicmmzJ"
      },
      "outputs": [],
      "source": [
        "# Create index_bucket if none is provided\n",
        "import uuid\n",
        "from google.cloud import storage\n",
        "\n",
        "storage_client = storage.Client()\n",
        "random_suffix = uuid.uuid4().hex[:6]  # Get a 6-character hexadecimal suffix\n",
        "index_bucket_name = f\"bm25-index-{random_suffix}\"\n",
        "\n",
        "if not index_bucket:\n",
        "    index_bucket = storage_client.create_bucket(index_bucket_name)\n",
        "    print(f\"Bucket {index_bucket.name} created.\")\n",
        "else:\n",
        "    print(f\"Using provided index_bucket: {index_bucket}\")\n",
        "    index_bucket = storage_client.bucket(index_bucket)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tRJqmmzPrAYR",
      "metadata": {
        "id": "tRJqmmzPrAYR"
      },
      "source": [
        "### Define Column Weights for BM25 Index\n",
        "\n",
        "This is a fairly brute-force method of adding more weight to one column over another by simply repeating it multiple times in the BM25 embedding input. You may prefer to research other more sophisticated methods to achieve similar results, but this gets the job done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3VJm91frAPc",
      "metadata": {
        "id": "a3VJm91frAPc"
      },
      "outputs": [],
      "source": [
        "# Higher weights give more priority to the column\n",
        "name_weight = 5\n",
        "brand_weight = 3\n",
        "category_weight = 2\n",
        "department_weight = 2\n",
        "product_description_weight = 1\n",
        "sku_weight = 1\n",
        "bm25_index_sql = f\"\"\"SELECT id,\n",
        "    REPEAT(COALESCE(name,'') || ' ', {name_weight}) ||\n",
        "    REPEAT(COALESCE(brand,'') || ' ', {brand_weight}) ||\n",
        "    REPEAT(COALESCE(category,'') || ' ', {category_weight}) ||\n",
        "    REPEAT(COALESCE(department,'') || ' ', {department_weight}) ||\n",
        "    REPEAT(COALESCE(product_description,''), {product_description_weight})||\n",
        "    REPEAT(COALESCE(sku,'') || ' ', {sku_weight}) AS content\n",
        "    FROM products;\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MMZ6K6_8yArr",
      "metadata": {
        "id": "MMZ6K6_8yArr"
      },
      "source": [
        "### Build BM25 Index from AlloyDB Content\n",
        "\n",
        "> NOTE: In this example, we run `bm25_ef.fit()` in-memory and serve the model locally since we have a relatively small product catalog (29,120 items). For very large product catalogs, this operation may exceed local memory. In such case, you might need to re-write this step in Spark (or similar) and potentially host the model on a dedicated endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W22ETqksxZ7v",
      "metadata": {
        "id": "W22ETqksxZ7v"
      },
      "outputs": [],
      "source": [
        "# Reference: https://milvus.io/api-reference/pymilvus/v2.4.x/EmbeddingModels/BM25EmbeddingFunction/BM25EmbeddingFunction.md\n",
        "\n",
        "from pymilvus.model.sparse import BM25EmbeddingFunction\n",
        "from pymilvus.model.sparse.bm25.tokenizers import build_default_analyzer\n",
        "\n",
        "# Use the default English analyzer\n",
        "analyzer = build_default_analyzer(language=\"en\")\n",
        "\n",
        "# Instantiate BM25 model\n",
        "bm25_ef = BM25EmbeddingFunction(\n",
        "    analyzer = analyzer,\n",
        "    k1 = 1.5, # This controls document term normalization\n",
        "    b = 0.75, # This controls field length normalization\n",
        "    epsilon = 0.25 # This is used to smooth idf values\n",
        ")\n",
        "\n",
        "# Get text content from the vector store\n",
        "docs = await run_query(ecom_db_pool, bm25_index_sql)\n",
        "docs = docs.replace(\"'\",\"\")\n",
        "\n",
        "# Fit the model to the AlloyDB content\n",
        "bm25_ef.fit(docs['content'].to_list())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gjqzkDoPg47S",
      "metadata": {
        "id": "gjqzkDoPg47S"
      },
      "source": [
        "### Upload Model Parameters to GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DPvnEtngg8sv",
      "metadata": {
        "id": "DPvnEtngg8sv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Store the fitted parameters to expedite future processing.\n",
        "bm25_params_file_name = \"bm25_params.json\"\n",
        "bm25_ef.save(bm25_params_file_name)\n",
        "\n",
        "# Upload saved model parameters to GCS\n",
        "current_directory = os.getcwd()\n",
        "blob_path = f\"bm25_index/{bm25_params_file_name}\"\n",
        "file_name = f\"{current_directory}/{bm25_params_file_name}\"\n",
        "blob = index_bucket.blob(blob_path)\n",
        "blob.upload_from_filename(file_name)\n",
        "print(f\"Uploaded model parameters from: {file_name} to gs://{index_bucket.name}/{blob_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6u_pMLFOhEb7",
      "metadata": {
        "id": "6u_pMLFOhEb7"
      },
      "source": [
        "### Download Model Parameters from GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sXf_sjxv1etH",
      "metadata": {
        "id": "sXf_sjxv1etH"
      },
      "outputs": [],
      "source": [
        "# Download model parameters (update blob to use your own file)\n",
        "blob = index_bucket.blob(f\"bm25_index/{bm25_params_file_name}\")\n",
        "blob.download_to_filename(bm25_params_file_name)\n",
        "\n",
        "# Load the saved params (optionally provide your own index.json)\n",
        "bm25_ef = BM25EmbeddingFunction()\n",
        "bm25_ef.load(bm25_params_file_name)\n",
        "\n",
        "# Print out the max dims:\n",
        "max_1_based_dims = bm25_ef.dim + 1\n",
        "print(f\"Max dimensionality: {max_1_based_dims}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v5KJO4yrhKWg",
      "metadata": {
        "id": "v5KJO4yrhKWg"
      },
      "source": [
        "### Define Helper Function for `sparsevec` Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8PxQZ9pTY2ET",
      "metadata": {
        "id": "8PxQZ9pTY2ET"
      },
      "outputs": [],
      "source": [
        "def encode_sparsevec(query: str, dimensions: int = max_1_based_dims) -> str:\n",
        "\n",
        "    # Generate the sparse embeddings\n",
        "    sparse_embeddings = bm25_ef.encode_queries([query])\n",
        "    lil = sparse_embeddings.tolil(copy=False)\n",
        "    sparse_scores, sparse_indices = lil.data.tolist()[0], lil.rows.tolist()[0]\n",
        "\n",
        "    # Ensure sparse_scores and sparse_indices lists are the same length\n",
        "    assert len(sparse_scores) == len(sparse_indices)\n",
        "\n",
        "    # sparsevec data type is 1-based. sparse_indices are zero-based.\n",
        "    sparse_indices = [x + 1 for x in sparse_indices]\n",
        "\n",
        "    # Zip results and transform to expected format for pgvector sparsevec type\n",
        "    result = [f\"{key}:{value:.7g}\" for key, value in zip(sparse_indices, sparse_scores)]\n",
        "    return f\"{{{','.join(result)}}}/{dimensions}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cPlZztNZhSly",
      "metadata": {
        "id": "cPlZztNZhSly"
      },
      "source": [
        "### Create Sparse Embeddings for Content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mnt_LKRdayuK",
      "metadata": {
        "id": "mnt_LKRdayuK"
      },
      "outputs": [],
      "source": [
        "docs['sparse_embedding'] = docs['content'].apply(encode_sparsevec)\n",
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7xJnbaDlhcjJ",
      "metadata": {
        "id": "7xJnbaDlhcjJ"
      },
      "source": [
        "### Add Sparse Embedding Columns to AlloyDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yyj88ncahinz",
      "metadata": {
        "id": "yyj88ncahinz"
      },
      "outputs": [],
      "source": [
        "sql_array = []\n",
        "sql_array.append(f\"ALTER TABLE products DROP COLUMN IF EXISTS sparse_embedding;\")\n",
        "sql_array.append(f\"ALTER TABLE products ADD COLUMN sparse_embedding sparsevec({bm25_ef.dim + 1});\")\n",
        "sql_array.append(f\"ALTER TABLE products DROP COLUMN IF EXISTS sparse_embedding_model;\")\n",
        "sql_array.append(f\"ALTER TABLE products ADD COLUMN sparse_embedding_model TEXT;\")\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8PaGyAGzk0cd",
      "metadata": {
        "id": "8PaGyAGzk0cd"
      },
      "source": [
        "### Update AlloyDB with Sparse Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qvKkaXnzkzed",
      "metadata": {
        "id": "qvKkaXnzkzed"
      },
      "outputs": [],
      "source": [
        "# Use bulk executemany for more efficient update\n",
        "data_to_update = [\n",
        "    {\"sparse_embedding\": row.sparse_embedding, \"id\": row.id}\n",
        "    for row in docs.itertuples(index=False) # Use index=False since id is a column, not a df index\n",
        "]\n",
        "\n",
        "sql = f\"\"\"\n",
        "UPDATE products\n",
        "SET sparse_embedding = :sparse_embedding,\n",
        "    sparse_embedding_model = 'BM25'\n",
        "WHERE id = :id;\n",
        "\"\"\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql, data_to_update)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2X4_vFvEvkK",
      "metadata": {
        "id": "b2X4_vFvEvkK"
      },
      "source": [
        "## Create Weighted Full-text Search Column\n",
        "\n",
        "Columns weighted `A` have more weight than columns weighted `B`, which have more weight than columns weighted `C`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1MvpKrq0FSzC",
      "metadata": {
        "id": "1MvpKrq0FSzC"
      },
      "outputs": [],
      "source": [
        "sql_array = []\n",
        "sql_array.append(f\"ALTER TABLE products DROP COLUMN IF EXISTS fts_document;\")\n",
        "sql_array.append(f\"\"\"ALTER TABLE products ADD COLUMN fts_document tsvector GENERATED ALWAYS AS (\n",
        "      setweight(to_tsvector('english', coalesce(name, '')), 'A') ||\n",
        "      setweight(to_tsvector('english', coalesce(brand, '')), 'B') ||\n",
        "      setweight(to_tsvector('english', coalesce(category, '')), 'C') ||\n",
        "      setweight(to_tsvector('english', coalesce(department, '')), 'C') ||\n",
        "      setweight(to_tsvector('english', coalesce(product_description, '')), 'D') ||\n",
        "      setweight(to_tsvector('english', coalesce(sku, '')), 'D')\n",
        "    ) STORED;\"\"\")\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58VCvupDUaGR",
      "metadata": {
        "id": "58VCvupDUaGR"
      },
      "source": [
        "## Create Indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a8d4f4e",
      "metadata": {
        "id": "0a8d4f4e"
      },
      "source": [
        "### Create Standard PostgreSQL Indexes for Efficient Facet Searches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed69e4bc",
      "metadata": {
        "id": "ed69e4bc"
      },
      "outputs": [],
      "source": [
        "sql_array = []\n",
        "\n",
        "sql_array.append(\"DROP INDEX IF EXISTS idx_products_brand;\")\n",
        "sql_array.append(\"CREATE INDEX idx_products_brand ON products (brand);\")\n",
        "sql_array.append(\"DROP INDEX IF EXISTS idx_products_category;\")\n",
        "sql_array.append(\"CREATE INDEX idx_products_category ON products (category);\")\n",
        "sql_array.append(\"DROP INDEX IF EXISTS idx_products_retail_price;\")\n",
        "sql_array.append(\"CREATE INDEX idx_products_retail_price ON products (retail_price);\")\n",
        "sql_array.append(\"DROP INDEX IF EXISTS idx_products_sku;\")\n",
        "sql_array.append(\"CREATE INDEX idx_products_sku ON products (sku);\")\n",
        "\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nWcLpp16J4Lk",
      "metadata": {
        "id": "nWcLpp16J4Lk"
      },
      "source": [
        "### Create ScaNN Indexes for Efficient ANN Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NfzxLpI9J3r-",
      "metadata": {
        "id": "NfzxLpI9J3r-"
      },
      "outputs": [],
      "source": [
        "sql_array = []\n",
        "\n",
        "sql_array.append(\"CREATE EXTENSION IF NOT EXISTS alloydb_scann\")\n",
        "sql_array.append(\"SET SESSION scann.num_leaves_to_search = 1\")\n",
        "sql_array.append(\"SET SESSION scann.pre_reordering_num_neighbors=50\")\n",
        "sql_array.append(\"DROP INDEX IF EXISTS embedding_scann\")\n",
        "sql_array.append(\"\"\"\n",
        "CREATE INDEX embedding_scann ON products\n",
        "  USING scann (product_embedding cosine)\n",
        "  WITH (num_leaves=2);\n",
        "\"\"\")\n",
        "sql_array.append(\"DROP INDEX IF EXISTS product_description_embedding_scann\")\n",
        "sql_array.append(\"\"\"\n",
        "CREATE INDEX product_description_embedding_scann ON products\n",
        "  USING scann (product_description_embedding cosine)\n",
        "  WITH (num_leaves=2);\n",
        "\"\"\")\n",
        "sql_array.append(\"DROP INDEX IF EXISTS product_image_embedding_scann\")\n",
        "sql_array.append(\"\"\"\n",
        "CREATE INDEX product_image_embedding_scann ON products\n",
        "  USING scann (product_image_embedding cosine)\n",
        "  WITH (num_leaves=2);\n",
        "\"\"\")\n",
        "\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85vI3qRUXoNX",
      "metadata": {
        "id": "85vI3qRUXoNX"
      },
      "source": [
        "### Create HNSW Index for Efficient BM25 ANN Sparsevec Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XHL3CxYAUZsz",
      "metadata": {
        "id": "XHL3CxYAUZsz"
      },
      "outputs": [],
      "source": [
        "sql_array = []\n",
        "sql_array.append(\"DROP INDEX IF EXISTS sparse_embedding_hnsw\")\n",
        "sql_array.append(\"\"\"\n",
        "CREATE INDEX sparse_embedding_hnsw ON products\n",
        "  USING hnsw (sparse_embedding sparsevec_ip_ops)\n",
        "  WITH (m = 16, ef_construction = 64);\n",
        "\"\"\")\n",
        "sql_array.append(\"SET hnsw.ef_search = 100;\") # This is necessary for better recall with sparsevec\n",
        "\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xvfuwxg5XuXf",
      "metadata": {
        "id": "Xvfuwxg5XuXf"
      },
      "source": [
        "### Create GIN Index for Efficient Full-text Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZLnIrwdGXuHx",
      "metadata": {
        "id": "ZLnIrwdGXuHx"
      },
      "outputs": [],
      "source": [
        "# Ref: https://www.postgresql.org/docs/current/gin.html\n",
        "sql_array = []\n",
        "sql_array.append(\"DROP INDEX IF EXISTS products_fts_document_gin;\")\n",
        "sql_array.append(\"CREATE INDEX products_fts_document_gin ON products USING GIN (fts_document);\")\n",
        "\n",
        "for sql in sql_array:\n",
        "  await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kfzFluRmFENq",
      "metadata": {
        "id": "kfzFluRmFENq"
      },
      "source": [
        "### Analyze `products` Table\n",
        "\n",
        "Collect statistics about the contents of table now that we've changed it substantially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S1Gs70eNH4iF",
      "metadata": {
        "id": "S1Gs70eNH4iF"
      },
      "outputs": [],
      "source": [
        "sql = \"ANALYZE products;\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2TUybjJgC4cV",
      "metadata": {
        "id": "2TUybjJgC4cV"
      },
      "source": [
        "## Run Advanced Product Search"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z_FOd2GfD_n9",
      "metadata": {
        "id": "Z_FOd2GfD_n9"
      },
      "source": [
        "### View the Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CmVKiXjDD_h6",
      "metadata": {
        "id": "CmVKiXjDD_h6"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT * FROM products LIMIT 5;\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k4dTATY0k1ls",
      "metadata": {
        "id": "k4dTATY0k1ls"
      },
      "source": [
        "### Search with Traditional SQL\n",
        "\n",
        "Traditional SQL remains one of the most popular and prevalent query languages among Business Analysts, Developers, DBAs, and Data Scientists due to its simplicity, expressiveness, and broad applicability. However, it is not usually the best choice for Product Search for many reasons, including:\n",
        "- Inefficient query patterns (e.g. searching multiple fields with ILIKE '%keyword%') will result in full table scans.\n",
        "- Intolerant to typos, synonyms, spelling variants, etc.\n",
        "- No relevance ranking (keyword proximity, field weighting, popularity, sales data, recency, user reviews, personalization signals) - result order is arbitrary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6kq1Jbwik1dA",
      "metadata": {
        "id": "6kq1Jbwik1dA"
      },
      "outputs": [],
      "source": [
        "# --- Uncomment one query at a time ---\n",
        "query = \"sunglasses\" # Simple keyword search returns relevant but arbitrarily ranked and inefficient results\n",
        "#query = \"silver sunglasses\" # Adding color returns NO results\n",
        "#query = \"sunglases\" # Typo returns NO results\n",
        "#query = \"81A6F51D90AF2C00DFC715C5DC5FE88D\" # Search by SKU returns the specific item, but inefficiently\n",
        "\n",
        "# --- Set explain=True to view query performance ---\n",
        "explain = False\n",
        "\n",
        "sql = f\"\"\"\n",
        "WITH trad_sql AS (\n",
        "    SELECT\n",
        "      ROW_NUMBER () OVER (ORDER BY name) AS trad_sql_rank,\n",
        "      name,\n",
        "      product_image_uri,\n",
        "      brand,\n",
        "      product_description,\n",
        "      category,\n",
        "      department,\n",
        "      cost,\n",
        "      retail_price,\n",
        "      sku\n",
        "    FROM products\n",
        "    WHERE name ILIKE '%{query}%'\n",
        "      OR sku ILIKE '%{query}%'\n",
        "      OR category ILIKE '%{query}%'\n",
        "      OR brand ILIKE '%{query}%'\n",
        "      OR department ILIKE '%{query}%'\n",
        "      OR product_description ILIKE '%{query}%'\n",
        "    ORDER BY name\n",
        "    LIMIT 10\n",
        ") SELECT * FROM trad_sql;\n",
        "\"\"\"\n",
        "\n",
        "if explain:\n",
        "  sql = 'EXPLAIN ANALYZE ' + sql\n",
        "\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "try:\n",
        "  html_output = HTML(result.to_html(escape=False, formatters={'product_image_uri': df_image_formatter}))\n",
        "  display(html_output)\n",
        "except:\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "leUmYM9uDEuL",
      "metadata": {
        "id": "leUmYM9uDEuL"
      },
      "source": [
        "### Search with Full-text Search\n",
        "\n",
        "Full-text Search in PostgreSQL provides the capability to identify natural-language documents that satisfy a query, and optionally to sort them by relevance to the query. The simplest search considers query as a set of words and similarity as the frequency of query words in the document.\n",
        "\n",
        "Full-text search provides several advantages over traditional SQL, including:\n",
        "- Significant performance improvement with the ts_vector data type (storing lexemes) and GIN indexes compared to the `ILIKE '%keyword%'` syntax.\n",
        "- Linguistic processing during indexing and querying, including stemming, stop word removal, and case normalization.\n",
        "- Relevance ranking with `ts_rank()` (based on term frequency) and `ts_rank_cd()` (based on term frequency and proximity of matching lexemes).\n",
        "- Weighted search, allowing you to prioritize matches found in one column (e.g. `name` or `sku`) over another (eg. `product_description`).\n",
        "\n",
        "Weaknesses:\n",
        "- Matches on lexical similarity, but does not understand semantic similarity.\n",
        "\n",
        "References:\n",
        "- https://www.postgresql.org/docs/current/textsearch-intro.html\n",
        "- https://www.postgresql.org/docs/current/functions-textsearch.html\n",
        "- https://www.postgresql.org/docs/current/textsearch-controls.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfR1oEChDEln",
      "metadata": {
        "id": "dfR1oEChDEln"
      },
      "outputs": [],
      "source": [
        "# --- Choose appropriate query function ---\n",
        "# plainto_tsquery: treats input as space-separated keywords, ANDs them. Good start.\n",
        "# websearch_to_tsquery: More flexible, handles quotes for phrases, OR, '-' for negation. Often better for user input.\n",
        "# phraseto_tsquery: Treats entire input as a phrase.\n",
        "# We'll use plainto_tsquery here, adjust as needed.\n",
        "\n",
        "fts_query_function = 'plainto_tsquery'\n",
        "\n",
        "# --- Uncomment one query at a time ---\n",
        "query = \"sunglasses\" # Simple keyword search returns relevant, ranked results ~200x faster than Traditional SQL\n",
        "#query = \"silver sunglasses\" # Adding color returns relevant, ranked, efficient results\n",
        "#query = \"sunglases\" # Typo returns NO results\n",
        "#query = \"sunglass\" # Stemmed lexeme returns results\n",
        "#query = \"81A6F51D90AF2C00DFC715C5DC5FE88D\" # Search by SKU efficiently returns the specific item\n",
        "#query = \"Shades\" # FTS doesn't understand that 'shades' are semantically similar to 'sunglasses'\n",
        "\n",
        "# --- Set explain=True to view query performance ---\n",
        "explain = False\n",
        "\n",
        "sql = f\"\"\"\n",
        "WITH fts_search AS (\n",
        "  SELECT\n",
        "      ts_rank(fts_document, {fts_query_function}('english', '{query}')) AS fts_rank_score,\n",
        "      name,\n",
        "      product_image_uri,\n",
        "      brand,\n",
        "      product_description,\n",
        "      category,\n",
        "      department,\n",
        "      cost,\n",
        "      retail_price,\n",
        "      sku\n",
        "  FROM products\n",
        "  WHERE fts_document @@ {fts_query_function}('english', '{query}')\n",
        "  ORDER BY fts_rank_score DESC\n",
        "  LIMIT 10\n",
        ") SELECT\n",
        "  ROW_NUMBER () OVER (ORDER BY fts_rank_score DESC) AS fts_rank, *\n",
        "  FROM fts_search\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "if explain:\n",
        "  sql = 'EXPLAIN ANALYZE ' + sql\n",
        "\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "try:\n",
        "  html_output = HTML(result.to_html(escape=False, formatters={'product_image_uri': df_image_formatter}))\n",
        "  display(html_output)\n",
        "except:\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uXtkvpWTC9HV",
      "metadata": {
        "id": "uXtkvpWTC9HV"
      },
      "source": [
        "### Search with BM25 Keyword Search\n",
        "\n",
        "BM25 (Okapi BM25) is the default ranking algorithm in many dedicated search engines and is generally considered a more advanced and often more effective  solution than full-text search, due in part to the following characteristics:\n",
        "- Term Frequency (TF) Saturation: PostgreSQL's ts_rank relevance score increases linearly with the number of times a term appears in a document. This means a document repeating a keyword many times can get a disproportionately high score. BM 25 implements TF saturation using a parameter (`k1`). This means that after a term appears a certain number of times, each additional occurrence contributes less and less to the score. This better reflects human intuition (a document mentioning \"sunglasses\" 5 times is relevant, but one mentioning it 50 times isn't necessarily 10 times more relevant) and prevents keyword stuffing from dominating results.\n",
        "- Document Length Normalization: PostgreSQL's ts_rank includes normalization options to penalize longer documents (controlled by flags), but the method is relatively simple. BM25 uses a more sophisticated normalization method controlled by a parameter (`b`). It considers the document's length relative to the average document length across the entire collection. This allows it to more effectively balance relevance across documents of varying lengths, often preventing very long or very short documents from being unfairly penalized or promoted.\n",
        "- Tunability: PostgreSQL's ts_rank allows tuning by  adjusting normalization flags or using `setweight` for different fields. BM25 provides explicit parameters (`k1` for TF saturation, `b` for length normalization) that directly control the core behavior of the ranking algorithm, allowing for fine-tuning based on the characteristics of the specific dataset and desired search behavior.\n",
        "- Empirical Performance: BM25 is derived from probabilistic models and has consistently demonstrated strong performance in information retrieval benchmarks, often outperforming standard TF-IDF implementations in terms of relevance ranking quality.\n",
        "\n",
        "The BM25 index input for this example was defined as:\n",
        "\n",
        "```sql\n",
        "SELECT COALESCE(name,'') || ' ' ||\n",
        "       COALESCE(sku,'') || ' ' ||\n",
        "       COALESCE(category,'') || ' ' ||\n",
        "       COALESCE(brand,'') || ' ' ||\n",
        "       COALESCE(department,'') || ' ' ||\n",
        "       COALESCE(product_description,'')\n",
        "FROM products;\n",
        "```\n",
        "\n",
        "Weaknesses\n",
        "- New vocabulary requires rebuilding the index and re-embedding every record.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AQ7DnFry0nj4",
      "metadata": {
        "id": "AQ7DnFry0nj4"
      },
      "outputs": [],
      "source": [
        "# --- Set explain=True to view query performance ---\n",
        "explain = False\n",
        "\n",
        "# --- Uncomment one query at a time ---\n",
        "query = \"sunglasses\" # Simple keyword search gives good results\n",
        "#query = \"silver sunglasses\" # Lots of irrelevant results with 'silver' in the name\n",
        "#query = \"81A6F51D90AF2C00DFC715C5DC5FE88D\" # Search by SKU does not return relevant results\n",
        "#query = \"silver sunglasses\" # Adding color returns non-relevant results\n",
        "#query = \"Shades\" # BM25 also doesn't understand that 'shades' are semantically similar to 'sunglasses'\n",
        "\n",
        "sparse_query_embedding = encode_sparsevec(query)\n",
        "print(f\"Encoded query as BM25 sparse embedding: {sparse_query_embedding}\")\n",
        "\n",
        "sql = f\"\"\"\n",
        "WITH bm25_search AS (\n",
        "  SELECT sparse_embedding <#> '{sparse_query_embedding}' AS bm25_distance,\n",
        "      ROW_NUMBER () OVER (ORDER BY sparse_embedding <#> '{sparse_query_embedding}') AS bm25_rank,\n",
        "      name,\n",
        "      product_image_uri,\n",
        "      brand,\n",
        "      product_description,\n",
        "      category,\n",
        "      department,\n",
        "      cost,\n",
        "      retail_price,\n",
        "      sku\n",
        "    FROM products\n",
        "    ORDER BY bm25_distance\n",
        "    LIMIT 10\n",
        ") SELECT * FROM bm25_search WHERE bm25_distance < 1;\n",
        "\"\"\"\n",
        "if explain:\n",
        "  sql = 'EXPLAIN ANALYZE ' + sql\n",
        "\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "try:\n",
        "  html_output = HTML(result.to_html(escape=False, formatters={'product_image_uri': df_image_formatter}))\n",
        "  display(html_output)\n",
        "except:\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8QBsLgK5DP8J",
      "metadata": {
        "id": "8QBsLgK5DP8J"
      },
      "source": [
        "### Search with Text Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dngmOhmu8P7m",
      "metadata": {
        "id": "dngmOhmu8P7m"
      },
      "source": [
        "#### `gemini-embedding-001`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zuJLHWkVDP2F",
      "metadata": {
        "id": "zuJLHWkVDP2F"
      },
      "outputs": [],
      "source": [
        "# --- Set explain=True to view query performance ---\n",
        "explain = False\n",
        "\n",
        "# --- Uncomment one query at a time ---\n",
        "query = \"sunglasses\" # Simple keyword search gives good results\n",
        "#query = \"silver sunglasses\" # All results are for sunglasses\n",
        "#query = \"81A6F51D90AF2C00DFC715C5DC5FE88D\" # Search by SKU does not return relevant results\n",
        "#query = \"silver sunglasses\" # Adding color returns all sunglasses, but misses two results that FTS found\n",
        "#query = \"Shades\" # gemini-embedding-001 model understands that 'shades' is semanitcally similar to 'sunglasses'\n",
        "\n",
        "sql = f\"\"\"\n",
        "WITH vector_search AS (\n",
        "  SELECT product_embedding <=> embedding('gemini-embedding-001', '{query}')::vector AS distance,\n",
        "    name,\n",
        "    product_image_uri,\n",
        "    brand,\n",
        "    product_description,\n",
        "    category,\n",
        "    department,\n",
        "    cost,\n",
        "    retail_price,\n",
        "    sku\n",
        "  FROM products\n",
        "  ORDER BY distance\n",
        "  LIMIT 10\n",
        ") SELECT ROW_NUMBER () OVER (ORDER BY distance) AS vector_rank, *\n",
        "FROM vector_search\n",
        "\"\"\"\n",
        "\n",
        "if explain:\n",
        "  sql = 'EXPLAIN ANALYZE ' + sql\n",
        "\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "try:\n",
        "  html_output = HTML(result.to_html(escape=False, formatters={'product_image_uri': df_image_formatter}))\n",
        "  display(html_output)\n",
        "except:\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6voI7G5g8UNV",
      "metadata": {
        "id": "6voI7G5g8UNV"
      },
      "source": [
        "#### `multimodal-embedding@001`\n",
        "\n",
        "> NOTE: This is very inaccurate right now. Need to re-embed with different input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XKYQ8KEP8UGQ",
      "metadata": {
        "id": "XKYQ8KEP8UGQ"
      },
      "outputs": [],
      "source": [
        "# --- Set explain=True to view query performance ---\n",
        "explain = False\n",
        "\n",
        "# --- Uncomment one query at a time ---\n",
        "#query = \"sunglasses\" # TBD\n",
        "query = \"silver sunglasses\" # TBD\n",
        "#query = \"81A6F51D90AF2C00DFC715C5DC5FE88D\" # TBD\n",
        "#query = \"silver sunglasses\" # TBD\n",
        "#query = \"Shades\" # TBD\n",
        "\n",
        "\n",
        "# Get embedding\n",
        "multimodal_embedding = generate_multimodal_embeddings(None, query)\n",
        "\n",
        "# Search\n",
        "sql = f\"\"\"\n",
        "WITH multimodal_vector_search AS (\n",
        "  SELECT product_description_embedding <=> '{multimodal_embedding.text_embedding}' AS distance,\n",
        "    name,\n",
        "    product_image_uri,\n",
        "    brand,\n",
        "    product_description,\n",
        "    category,\n",
        "    department,\n",
        "    cost,\n",
        "    retail_price,\n",
        "    sku\n",
        "  FROM products\n",
        "  ORDER BY distance\n",
        "  LIMIT 10\n",
        ") SELECT RANK () OVER (ORDER BY distance) AS vector_rank, *\n",
        "FROM multimodal_vector_search\n",
        "\"\"\"\n",
        "\n",
        "if explain:\n",
        "  sql = 'EXPLAIN ANALYZE ' + sql\n",
        "\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "try:\n",
        "  html_output = HTML(result.to_html(escape=False, formatters={'product_image_uri': df_image_formatter}))\n",
        "  display(html_output)\n",
        "except:\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o9MN6yx3DTRL",
      "metadata": {
        "id": "o9MN6yx3DTRL"
      },
      "source": [
        "### Search by Image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j0ym2Jupkyia",
      "metadata": {
        "id": "j0ym2Jupkyia"
      },
      "source": [
        "#### Show sample images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zTooWMqTgrnu",
      "metadata": {
        "id": "zTooWMqTgrnu"
      },
      "outputs": [],
      "source": [
        "test_images = {\n",
        "  'object': ['Blue Jacket', 'Brown Jacket', 'Black Coat', 'Gray Jacket', 'Noogler Hat',],\n",
        "  'gs_uri': ['gs://pr-public-demo-data/alloydb-retail-demo/user_photos/1.png',\n",
        "             'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/2.png',\n",
        "             'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/3.png',\n",
        "             'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/4.png',\n",
        "             'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/5.png'],\n",
        "  'image': ['https://storage.cloud.google.com/pr-public-demo-data/alloydb-retail-demo/user_photos/1.png',\n",
        "            'https://storage.cloud.google.com/pr-public-demo-data/alloydb-retail-demo/user_photos/2.png',\n",
        "            'https://storage.cloud.google.com/pr-public-demo-data/alloydb-retail-demo/user_photos/3.png',\n",
        "            'https://storage.cloud.google.com/pr-public-demo-data/alloydb-retail-demo/user_photos/4.png',\n",
        "            'https://storage.cloud.google.com/pr-public-demo-data/alloydb-retail-demo/user_photos/5.png']\n",
        "}\n",
        "\n",
        "test_images_df = pd.DataFrame(test_images)\n",
        "test_images_df\n",
        "HTML(test_images_df.to_html(escape=False, formatters={'image': lambda x: df_image_formatter(x, 400)}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jhqMlJqfmLL4",
      "metadata": {
        "id": "jhqMlJqfmLL4"
      },
      "source": [
        "#### Search by Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hm7ND2iTmLAB",
      "metadata": {
        "id": "Hm7ND2iTmLAB"
      },
      "outputs": [],
      "source": [
        "# Uncomment 1 image at a time.\n",
        "image_uri = 'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/1.png'\n",
        "#image_uri = 'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/2.png'\n",
        "#image_uri = 'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/3.png'\n",
        "#image_uri = 'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/4.png'\n",
        "#image_uri = 'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/5.png'\n",
        "\n",
        "multimodal_embedding = generate_multimodal_embeddings(image_uri, None)\n",
        "\n",
        "sql = f\"\"\"\n",
        "WITH multimodal_vector_search AS (\n",
        "  SELECT product_image_embedding <=> '{multimodal_embedding.image_embedding}' AS distance,\n",
        "    id,\n",
        "    name,\n",
        "    product_image_uri,\n",
        "    brand,\n",
        "    product_description,\n",
        "    category,\n",
        "    department,\n",
        "    cost,\n",
        "    retail_price,\n",
        "    sku\n",
        "  FROM products\n",
        "  ORDER BY distance\n",
        "  LIMIT 24\n",
        ") SELECT RANK () OVER (ORDER BY distance) AS vector_rank, *\n",
        "FROM multimodal_vector_search\n",
        "\"\"\"\n",
        "\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "\n",
        "HTML(result.to_html(escape=False, formatters={'product_image_uri': df_image_formatter}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rq4qG00jERzL",
      "metadata": {
        "id": "Rq4qG00jERzL"
      },
      "source": [
        "#### Use AlloyDB Multimodal Embedding Function\n",
        "\n",
        "Instead of generating the embedding first and then executing the SQL query, you can generate the image embedding for the query on the fly with SQL using AlloyDB's new multimodal embedding feature.\n",
        "\n",
        "> NOTE: This is a preview feature. See [this doc](https://cloud.google.com/alloydb/docs/ai/generate-multimodal-embeddings) for more info about getting access to the feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MeE6O4KEER9T",
      "metadata": {
        "id": "MeE6O4KEER9T"
      },
      "outputs": [],
      "source": [
        "# Reference: https://cloud.google.com/alloydb/docs/ai/generate-multimodal-embeddings\n",
        "\n",
        "image_uri = 'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/1.png'\n",
        "#image_uri = 'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/2.png'\n",
        "#image_uri = 'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/3.png'\n",
        "#image_uri = 'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/4.png'\n",
        "#image_uri = 'gs://pr-public-demo-data/alloydb-retail-demo/user_photos/5.png'\n",
        "\n",
        "\n",
        "sql = f\"\"\"\n",
        "WITH image_embedding AS (\n",
        "  SELECT ai.image_embedding(\n",
        "      model_id => 'multimodalembedding@001',\n",
        "      image => '{image_uri}',\n",
        "      mimetype => 'image/png')::vector AS embedding\n",
        "), multimodal_vector_search AS (\n",
        "  SELECT product_image_embedding <=> image_embedding.embedding AS distance,\n",
        "    name,\n",
        "    product_image_uri,\n",
        "    brand,\n",
        "    product_description,\n",
        "    category,\n",
        "    department,\n",
        "    cost,\n",
        "    retail_price,\n",
        "    sku\n",
        "  FROM products, image_embedding\n",
        "  ORDER BY distance\n",
        "  LIMIT 5\n",
        ") SELECT RANK () OVER (ORDER BY distance) AS vector_rank, *\n",
        "FROM multimodal_vector_search\n",
        "\"\"\"\n",
        "\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "#result\n",
        "\n",
        "HTML(result.to_html(escape=False, formatters={'product_image_uri': df_image_formatter}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "buGPSDPY2XRJ",
      "metadata": {
        "id": "buGPSDPY2XRJ"
      },
      "source": [
        "### Combine Search Techniques with Hybrid Search\n",
        "\n",
        "In testing, this efficient hybrid search query ran in less than 20ms.\n",
        "\n",
        "> NOTE: Traditional SQL was modified to include just SKU matching for efficiency. Vector, B25, and FTS techniques are implemented as shown above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g6YftJ5ojbWG",
      "metadata": {
        "id": "g6YftJ5ojbWG"
      },
      "outputs": [],
      "source": [
        "# TO DO:  Fix search on misspellings. Currently returning all BM25 irrelevant results.\n",
        "\n",
        "# --- Set explain=True to view query performance ---\n",
        "explain = False\n",
        "\n",
        "# --- Uncomment one query at a time ---\n",
        "query = \"sunglasses\" # Simple keyword search gives good results\n",
        "#query = \"silver sunglasses\" # We get more sunglasses than non-sunglasses in the result\n",
        "#query = \"81A6F51D90AF2C00DFC715C5DC5FE88D\" # First result is our exact SKU\n",
        "#query = \"Shades\" # Blended result\n",
        "query = \"sunglases\" # Misspelling\n",
        "\n",
        "sparse_query_embedding = encode_sparsevec(query)\n",
        "\n",
        "print(f\"BM25 sparse embedding: {sparse_query_embedding}\")\n",
        "\n",
        "# Set top_k\n",
        "top_k = 20\n",
        "\n",
        "# Define RRF smoothing constant k\n",
        "rrf_k = 60\n",
        "\n",
        "sql = f\"\"\"\n",
        "WITH trad_sql AS (\n",
        "    SELECT\n",
        "      RANK () OVER (ORDER BY name) AS trad_sql_rank,\n",
        "      id,\n",
        "      name,\n",
        "      product_image_uri,\n",
        "      brand,\n",
        "      product_description,\n",
        "      category,\n",
        "      department,\n",
        "      cost,\n",
        "      retail_price,\n",
        "      sku\n",
        "    FROM products\n",
        "    WHERE sku = '{query}'\n",
        "    ORDER BY name\n",
        "    LIMIT {top_k * 2}\n",
        "), fts_search AS (\n",
        "  SELECT\n",
        "      ts_rank(fts_document, {fts_query_function}('english', '{query}')) AS fts_rank_score,\n",
        "      RANK () OVER (ORDER BY ts_rank(fts_document, {fts_query_function}('english', '{query}')) DESC) as fts_rank,\n",
        "      id,\n",
        "      name,\n",
        "      product_image_uri,\n",
        "      brand,\n",
        "      product_description,\n",
        "      category,\n",
        "      department,\n",
        "      cost,\n",
        "      retail_price,\n",
        "      sku\n",
        "  FROM products\n",
        "  WHERE fts_document @@ {fts_query_function}('english', '{query}')\n",
        "  ORDER BY fts_rank_score DESC\n",
        "  LIMIT {top_k * 2}\n",
        "), bm25_search AS (\n",
        "  SELECT sparse_embedding <#> '{sparse_query_embedding}' AS bm25_distance,\n",
        "      RANK () OVER (ORDER BY sparse_embedding <#> '{sparse_query_embedding}') AS bm25_rank,\n",
        "      id,\n",
        "      name,\n",
        "      product_image_uri,\n",
        "      brand,\n",
        "      product_description,\n",
        "      category,\n",
        "      department,\n",
        "      cost,\n",
        "      retail_price,\n",
        "      sku\n",
        "    FROM products\n",
        "    WHERE sparse_embedding <#> '{sparse_query_embedding}' < 1\n",
        "    ORDER BY sparse_embedding <#> '{sparse_query_embedding}'\n",
        "    LIMIT {top_k * 2}\n",
        "), vector_search AS (\n",
        "  SELECT embedding <=> embedding('gemini-embedding-001', '{query}')::vector AS distance,\n",
        "    RANK () OVER (ORDER BY embedding <=> embedding('gemini-embedding-001', '{query}')::vector) AS vector_rank,\n",
        "    id,\n",
        "    name,\n",
        "    product_image_uri,\n",
        "    brand,\n",
        "    product_description,\n",
        "    category,\n",
        "    department,\n",
        "    cost,\n",
        "    retail_price,\n",
        "    sku\n",
        "  FROM products\n",
        "  ORDER BY distance\n",
        "  LIMIT {top_k * 2}\n",
        ") SELECT\n",
        "    COALESCE(vector_search.id, fts_search.id, bm25_search.id, trad_sql.id) AS id,\n",
        "    (\n",
        "      COALESCE( (1.0 / ({rrf_k} + MAX(vector_search.vector_rank))), 0.0 ) +\n",
        "      COALESCE( (1.0 / ({rrf_k} + MAX(fts_search.fts_rank))), 0.0 ) +\n",
        "      COALESCE( (1.0 / ({rrf_k} + MAX(bm25_search.bm25_rank))), 0.0 ) +\n",
        "      COALESCE( (1.0 / ({rrf_k} + MAX(trad_sql.trad_sql_rank))), 0.0 )\n",
        "    ) AS rrf_score,\n",
        "    CONCAT_WS(\n",
        "        '+',\n",
        "        CASE WHEN MAX(vector_search.vector_rank) IS NOT NULL THEN 'vector' ELSE NULL END,\n",
        "        CASE WHEN MAX(fts_search.fts_rank) IS NOT NULL THEN 'fts' ELSE NULL END,\n",
        "        CASE WHEN MAX(bm25_search.bm25_rank) IS NOT NULL THEN 'bm25' ELSE NULL END,\n",
        "        CASE WHEN MAX(trad_sql.trad_sql_rank) IS NOT NULL THEN 'trad' ELSE NULL END\n",
        "    ) AS result_type,\n",
        "    COALESCE(vector_search.id, fts_search.id, bm25_search.id, trad_sql.id) AS id,\n",
        "    COALESCE(vector_search.name, fts_search.name, bm25_search.name, trad_sql.name) AS name,\n",
        "    COALESCE(vector_search.product_image_uri, fts_search.product_image_uri, bm25_search.product_image_uri, trad_sql.product_image_uri) AS product_image_uri,\n",
        "    COALESCE(vector_search.brand, fts_search.brand, bm25_search.brand, trad_sql.brand) AS brand,\n",
        "    COALESCE(vector_search.product_description, fts_search.product_description, bm25_search.product_description, trad_sql.product_description) AS product_description,\n",
        "    COALESCE(vector_search.category, fts_search.category, bm25_search.category, trad_sql.category) AS category,\n",
        "    COALESCE(vector_search.department, fts_search.department, bm25_search.department, trad_sql.department) AS department,\n",
        "    COALESCE(vector_search.cost, fts_search.cost, bm25_search.cost, trad_sql.cost) AS cost,\n",
        "    COALESCE(vector_search.retail_price, fts_search.retail_price, bm25_search.retail_price, trad_sql.retail_price) AS retail_price,\n",
        "    COALESCE(vector_search.sku, fts_search.sku, bm25_search.sku, trad_sql.sku) AS sku\n",
        "FROM vector_search\n",
        "FULL OUTER JOIN fts_search ON vector_search.id = fts_search.id\n",
        "FULL OUTER JOIN bm25_search ON COALESCE(vector_search.id, fts_search.id) = bm25_search.id\n",
        "FULL OUTER JOIN trad_sql ON COALESCE(vector_search.id, fts_search.id, bm25_search.id) = trad_sql.id\n",
        "GROUP BY COALESCE(vector_search.id, fts_search.id, bm25_search.id, trad_sql.id),\n",
        "    COALESCE(vector_search.name, fts_search.name, bm25_search.name, trad_sql.name),\n",
        "    COALESCE(vector_search.product_image_uri, fts_search.product_image_uri, bm25_search.product_image_uri, trad_sql.product_image_uri),\n",
        "    COALESCE(vector_search.brand, fts_search.brand, bm25_search.brand, trad_sql.brand),\n",
        "    COALESCE(vector_search.product_description, fts_search.product_description, bm25_search.product_description, trad_sql.product_description),\n",
        "    COALESCE(vector_search.category, fts_search.category, bm25_search.category, trad_sql.category),\n",
        "    COALESCE(vector_search.department, fts_search.department, bm25_search.department, trad_sql.department),\n",
        "    COALESCE(vector_search.cost, fts_search.cost, bm25_search.cost, trad_sql.cost),\n",
        "    COALESCE(vector_search.retail_price, fts_search.retail_price, bm25_search.retail_price, trad_sql.retail_price),\n",
        "    COALESCE(vector_search.sku, fts_search.sku, bm25_search.sku, trad_sql.sku)\n",
        "ORDER BY rrf_score DESC\n",
        "LIMIT {top_k};\n",
        "\"\"\"\n",
        "\n",
        "if explain:\n",
        "  sql = 'EXPLAIN ANALYZE ' + sql\n",
        "\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "try:\n",
        "  html_output = HTML(result.to_html(escape=False, formatters={'product_image_uri': df_image_formatter}))\n",
        "  display(html_output)\n",
        "except:\n",
        "  print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rgT0aEWdtX6P",
      "metadata": {
        "id": "rgT0aEWdtX6P"
      },
      "source": [
        "### Hybrid Search with Built-in Support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EaAPmu3jsJzi",
      "metadata": {
        "id": "EaAPmu3jsJzi"
      },
      "outputs": [],
      "source": [
        "# --- Choose appropriate query function ---\n",
        "# plainto_tsquery: treats input as space-separated keywords, ANDs them. Good start.\n",
        "# websearch_to_tsquery: More flexible, handles quotes for phrases, OR, '-' for negation. Often better for user input.\n",
        "# phraseto_tsquery: Treats entire input as a phrase.\n",
        "# We'll use plainto_tsquery here, adjust as needed.\n",
        "\n",
        "fts_query_function = 'websearch_to_tsquery'\n",
        "\n",
        "# --- Set explain=True to view query performance ---\n",
        "explain = False\n",
        "\n",
        "# --- Uncomment one query at a time ---\n",
        "query = \"sunglasses\" # Simple keyword search gives good results\n",
        "#query = \"silver sunglasses\" # We get more sunglasses than non-sunglasses in the result\n",
        "#query = \"81A6F51D90AF2C00DFC715C5DC5FE88D\" # First result is our exact SKU\n",
        "#query = \"Shades\" # Blended result\n",
        "#query = \"sunglases\" # Misspelling\n",
        "\n",
        "# Set top_k\n",
        "top_k = 20\n",
        "\n",
        "# Define RRF smoothing constant k\n",
        "rrf_k = 60\n",
        "\n",
        "sql = f\"\"\"\n",
        "WITH trad_sql AS (\n",
        "    SELECT\n",
        "      RANK () OVER (ORDER BY name) AS trad_sql_rank,\n",
        "      id,\n",
        "      name,\n",
        "      product_image_uri,\n",
        "      brand,\n",
        "      product_description,\n",
        "      category,\n",
        "      department,\n",
        "      cost,\n",
        "      retail_price,\n",
        "      sku,\n",
        "      'SQL' AS retrieval_method\n",
        "    FROM products\n",
        "    WHERE sku = '{query}'\n",
        "    ORDER BY name\n",
        "    LIMIT {top_k * 2}\n",
        "), fts_search AS (\n",
        "  SELECT\n",
        "      ts_rank(fts_document, {fts_query_function}('english', '{query}')) AS fts_rank_score,\n",
        "      RANK () OVER (ORDER BY ts_rank(fts_document, {fts_query_function}('english', '{query}')) DESC) as fts_rank,\n",
        "      id,\n",
        "      name,\n",
        "      product_image_uri,\n",
        "      brand,\n",
        "      product_description,\n",
        "      category,\n",
        "      department,\n",
        "      cost,\n",
        "      retail_price,\n",
        "      sku,\n",
        "      'FTS' AS retrieval_method\n",
        "  FROM products\n",
        "  WHERE fts_document @@ {fts_query_function}('english', '{query}')\n",
        "  ORDER BY fts_rank_score DESC\n",
        "  LIMIT {top_k * 2}\n",
        "), vector_search AS (\n",
        "  SELECT embedding <=> embedding('gemini-embedding-001', '{query}')::vector AS distance,\n",
        "      RANK () OVER (ORDER BY embedding <=> embedding('gemini-embedding-001', '{query}')::vector) AS vector_rank,\n",
        "      id,\n",
        "      name,\n",
        "      product_image_uri,\n",
        "      brand,\n",
        "      product_description,\n",
        "      category,\n",
        "      department,\n",
        "      cost,\n",
        "      retail_price,\n",
        "      sku,\n",
        "      'VECTOR' AS retrieval_method\n",
        "  FROM products\n",
        "  ORDER BY distance\n",
        "  LIMIT {top_k * 2}\n",
        ") SELECT\n",
        "    COALESCE(vector_search.id, fts_search.id, trad_sql.id) AS id,\n",
        "    (\n",
        "      COALESCE( (1.0 / ({rrf_k} + vector_search.vector_rank)), 0.0 ) +\n",
        "      COALESCE( (1.0 / ({rrf_k} + fts_search.fts_rank)), 0.0 ) +\n",
        "      COALESCE( (1.0 / ({rrf_k} + trad_sql.trad_sql_rank)), 0.0 )\n",
        "    ) AS rrf_score,\n",
        "    CONCAT_WS(\n",
        "        '+',\n",
        "        CASE WHEN vector_search.vector_rank IS NOT NULL THEN 'VECTOR' ELSE NULL END,\n",
        "        CASE WHEN fts_search.fts_rank IS NOT NULL THEN 'FTS' ELSE NULL END,\n",
        "        CASE WHEN trad_sql.trad_sql_rank IS NOT NULL THEN 'SQL' ELSE NULL END\n",
        "    ) AS result_type,\n",
        "    COALESCE(vector_search.id, fts_search.id, trad_sql.id) AS id,\n",
        "    COALESCE(vector_search.name, fts_search.name, trad_sql.name) AS name,\n",
        "    COALESCE(vector_search.product_image_uri, fts_search.product_image_uri, trad_sql.product_image_uri) AS product_image_uri,\n",
        "    COALESCE(vector_search.brand, fts_search.brand, trad_sql.brand) AS brand,\n",
        "    COALESCE(vector_search.product_description, fts_search.product_description, trad_sql.product_description) AS product_description,\n",
        "    COALESCE(vector_search.category, fts_search.category, trad_sql.category) AS category,\n",
        "    COALESCE(vector_search.department, fts_search.department, trad_sql.department) AS department,\n",
        "    COALESCE(vector_search.cost, fts_search.cost, trad_sql.cost) AS cost,\n",
        "    COALESCE(vector_search.retail_price, fts_search.retail_price, trad_sql.retail_price) AS retail_price,\n",
        "    COALESCE(vector_search.sku, fts_search.sku, trad_sql.sku) AS sku\n",
        "FROM vector_search\n",
        "FULL OUTER JOIN fts_search ON vector_search.id = fts_search.id\n",
        "FULL OUTER JOIN trad_sql ON COALESCE(vector_search.id, fts_search.id) = trad_sql.id\n",
        "ORDER BY rrf_score DESC\n",
        "LIMIT {top_k};\n",
        "\"\"\"\n",
        "\n",
        "if explain:\n",
        "  sql = 'EXPLAIN ANALYZE ' + sql\n",
        "\n",
        "print(sql)\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "try:\n",
        "  html_output = HTML(result.to_html(escape=False, formatters={'product_image_uri': df_image_formatter}))\n",
        "  display(html_output)\n",
        "except:\n",
        "  print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61210c2e",
      "metadata": {
        "id": "61210c2e"
      },
      "source": [
        "### Enable The AlloyDB Columnar Engine\n",
        "\n",
        "References:\n",
        "* https://cloud.google.com/alloydb/docs/columnar-engine/configure\n",
        "* https://cloud.google.com/alloydb/docs/instance-configure-database-flags#gcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8faeab9",
      "metadata": {
        "id": "f8faeab9"
      },
      "outputs": [],
      "source": [
        "result = ! gcloud beta alloydb instances update {alloydb_instance} \\\n",
        "   --database-flags google_columnar_engine.enabled=on,google_columnar_engine.enable_vectorized_join=on,password.enforce_complexity=on,google_ml_integration.enable_model_support=on \\\n",
        "   --region={region} \\\n",
        "   --cluster={alloydb_cluster} \\\n",
        "   --project={project_id} \\\n",
        "   --update-mode=FORCE_APPLY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1e90582",
      "metadata": {
        "id": "a1e90582"
      },
      "source": [
        "### Add Facet Columns to Column Store\n",
        "\n",
        "Reference: https://cloud.google.com/alloydb/docs/columnar-engine/manage-content-manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b20b65",
      "metadata": {
        "id": "a1b20b65"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"SELECT google_columnar_engine_add(\n",
        "    relation => 'products',\n",
        "    columns => 'id,brand,category,retail_price'\n",
        ");\"\"\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a20f809",
      "metadata": {
        "id": "8a20f809"
      },
      "source": [
        "### Validate Columns are Added to Columnar Engine\n",
        "\n",
        "Reference: https://cloud.google.com/alloydb/docs/columnar-engine/monitor-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0baafdb",
      "metadata": {
        "id": "d0baafdb"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT * FROM g_columnar_columns;\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fc6b1c2",
      "metadata": {
        "id": "9fc6b1c2"
      },
      "source": [
        "### Get Facets with Hybrid Search\n",
        "\n",
        "This example shows how you can efficiently get facets that show the count of brand, category, and price bin foryour hybrid search queries.\n",
        "\n",
        "> NOTE: The Columnar Engine may not be chosen for aggregations when existing indexes are more efficient. The query engine will decide whether using the Columnar Engine would be more efficient than a standard query or not. You can get details about the decisions the query engine is making in relation to choose the Columnar Engine by running `EXPLAIN (COLUMNAR_ENGINE)` on the query. For most queries on this small dataset (29,120 items), the Columnar Engine will not be chosen. For large product catalogs with millions of items, it's much more likely that the Columnar Engine will be chosen for efficient query execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3af075",
      "metadata": {
        "id": "1a3af075"
      },
      "outputs": [],
      "source": [
        "# --- Set explain=True to view query performance ---\n",
        "explain = False\n",
        "\n",
        "# --- Turn columnar engine \"on\" or \"off\" to test performance impact of the columnar engine. ---\n",
        "columnar_engine_state = \"on\"\n",
        "\n",
        "# --- Choose a query term ---\n",
        "query = \"sunglasses\" # Simple keyword search gives good results\n",
        "#query = \"silver sunglasses\" # We get more sunglasses than non-sunglasses in the result\n",
        "#query = \"81A6F51D90AF2C00DFC715C5DC5FE88D\" # First result is our exact SKU\n",
        "#query = \"Shades\" # Blended result\n",
        "#query = \"sunglases\" # Misspelling\n",
        "\n",
        "await run_query(ecom_db_pool, f\"SET google_columnar_engine.enable_columnar_scan={columnar_engine_state};\")\n",
        "\n",
        "sql = f\"\"\"WITH\n",
        "  -- 1. Define the pool of candidate IDs exactly once\n",
        "  candidate_ids AS (\n",
        "    WITH vector_candidates AS (\n",
        "      SELECT id, embedding <=> embedding('gemini-embedding-001', '{query}')::vector AS distance FROM products ORDER BY distance LIMIT 500\n",
        "    )\n",
        "    SELECT id FROM vector_candidates WHERE distance < 0.4\n",
        "    UNION\n",
        "    SELECT id FROM products WHERE sku = '{query}'\n",
        "    UNION\n",
        "    SELECT id FROM products WHERE fts_document @@ websearch_to_tsquery('english', '{query}')\n",
        "  ),\n",
        "  -- 2. Join products with candidates and prepare facet columns (including calculated ones)\n",
        "  products_for_faceting AS (\n",
        "    SELECT\n",
        "      p.brand,\n",
        "      p.category,\n",
        "      CASE\n",
        "        WHEN p.retail_price < 50 THEN '$0 - $49.99'\n",
        "        WHEN p.retail_price >= 50 AND p.retail_price < 100 THEN '$50 - $99.99'\n",
        "        WHEN p.retail_price >= 100 AND p.retail_price < 250 THEN '$100 - $249.99'\n",
        "        WHEN p.retail_price >= 250 AND p.retail_price < 500 THEN '$250 - $499.99'\n",
        "        WHEN p.retail_price >= 500 THEN '$500+'\n",
        "        ELSE NULL\n",
        "      END AS price_range,\n",
        "      p.retail_price\n",
        "    FROM\n",
        "      products AS p\n",
        "      JOIN candidate_ids AS c ON p.id = c.id\n",
        "  ),\n",
        "  -- 3. Calculate Aggregations using GROUPING SETS\n",
        "  facet_aggregations AS (\n",
        "    SELECT\n",
        "      COALESCE(brand, category, price_range) AS facet_value,\n",
        "      CASE\n",
        "        WHEN GROUPING(brand) = 0 THEN 'brand'\n",
        "        WHEN GROUPING(category) = 0 THEN 'category'\n",
        "        WHEN GROUPING(price_range) = 0 THEN 'price_range'\n",
        "      END AS facet_type,\n",
        "      COUNT(*) AS count,\n",
        "      MIN(retail_price) as min_price_for_ordering\n",
        "    FROM\n",
        "      products_for_faceting\n",
        "    WHERE\n",
        "        brand IS NOT NULL OR category IS NOT NULL OR price_range IS NOT NULL\n",
        "    GROUP BY\n",
        "      GROUPING SETS (\n",
        "        (brand),\n",
        "        (category),\n",
        "        (price_range)\n",
        "      )\n",
        "  )\n",
        "-- 4. Final SELECT and ORDER BY from the aggregated results\n",
        "SELECT\n",
        "  facet_value,\n",
        "  facet_type,\n",
        "  count\n",
        "FROM\n",
        "  facet_aggregations\n",
        "ORDER BY\n",
        "  facet_type ASC,\n",
        "  CASE WHEN facet_type = 'price_range' THEN min_price_for_ordering ELSE NULL END ASC NULLS LAST, -- Handle potential NULL min_price\n",
        "  count DESC,\n",
        "  facet_value ASC;\"\"\"\n",
        "\n",
        "if explain:\n",
        "  sql = 'EXPLAIN (COLUMNAR_ENGINE) ' + sql\n",
        "\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "try:\n",
        "  html_output = HTML(result.to_html(escape=False, formatters={'product_image_uri': df_image_formatter}))\n",
        "  display(html_output)\n",
        "except:\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0887dd29",
      "metadata": {
        "id": "0887dd29"
      },
      "source": [
        "## Get Facets for Full Result Set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3658ced",
      "metadata": {
        "id": "d3658ced"
      },
      "source": [
        "### Enable The AlloyDB Columnar Engine\n",
        "\n",
        "References:\n",
        "* https://cloud.google.com/alloydb/docs/columnar-engine/configure\n",
        "* https://cloud.google.com/alloydb/docs/instance-configure-database-flags#gcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "648e78fd",
      "metadata": {
        "id": "648e78fd"
      },
      "outputs": [],
      "source": [
        "result = ! gcloud beta alloydb instances update {alloydb_instance} \\\n",
        "   --database-flags google_columnar_engine.enabled=on,google_columnar_engine.enable_vectorized_join=on,password.enforce_complexity=on,google_ml_integration.enable_model_support=on \\\n",
        "   --region={region} \\\n",
        "   --cluster={alloydb_cluster} \\\n",
        "   --project={project_id} \\\n",
        "   --update-mode=FORCE_APPLY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6225e12b",
      "metadata": {
        "id": "6225e12b"
      },
      "source": [
        "### Add Columns to Column Store Automatically\n",
        "\n",
        "Reference: https://cloud.google.com/alloydb/docs/columnar-engine/manage-content-recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18e09d26",
      "metadata": {
        "id": "18e09d26"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT google_columnar_engine_recommend();\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a26d8c22",
      "metadata": {
        "id": "a26d8c22"
      },
      "source": [
        "#### View Recommended Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b8e4250",
      "metadata": {
        "id": "3b8e4250"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT database_name, schema_name, relation_name, column_name FROM g_columnar_recommended_columns;\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05d2b41a",
      "metadata": {
        "id": "05d2b41a"
      },
      "source": [
        "### (OPTIONAL) Add Facet Columns to Column Store Manually\n",
        "\n",
        "Reference: https://cloud.google.com/alloydb/docs/columnar-engine/manage-content-manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ce7bff",
      "metadata": {
        "id": "e3ce7bff"
      },
      "outputs": [],
      "source": [
        "sql = \"\"\"SELECT google_columnar_engine_add(\n",
        "    relation => 'products',\n",
        "    columns => 'id,brand,category,retail_price'\n",
        ");\"\"\"\n",
        "\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12298c71",
      "metadata": {
        "id": "12298c71"
      },
      "source": [
        "### Validate Columns are Added to Columnar Engine\n",
        "\n",
        "Reference: https://cloud.google.com/alloydb/docs/columnar-engine/monitor-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9cb91d4",
      "metadata": {
        "id": "f9cb91d4"
      },
      "outputs": [],
      "source": [
        "sql = \"SELECT * FROM g_columnar_columns;\"\n",
        "await run_query(ecom_db_pool, sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9294179c",
      "metadata": {
        "id": "9294179c"
      },
      "source": [
        "### Facets with Hybrid Search\n",
        "\n",
        "This example shows how you can efficiently get facets that show the count of brand, category, and price bin foryour hybrid search queries.\n",
        "\n",
        "In testing, execution time ranged from 80ms-200ms with Columnar Engine turned off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cb01ac3",
      "metadata": {
        "id": "7cb01ac3"
      },
      "outputs": [],
      "source": [
        "# --- Set explain=True to view query performance ---\n",
        "explain = False\n",
        "\n",
        "# --- Turn columnar engine \"on\" or \"off\" to test performance impact of the columnar engine. ---\n",
        "columnar_engine_state = \"on\"\n",
        "\n",
        "# --- Choose a query term ---\n",
        "query = \"sunglasses\" # Simple keyword search gives good results\n",
        "#query = \"silver sunglasses\" # We get more sunglasses than non-sunglasses in the result\n",
        "#query = \"81A6F51D90AF2C00DFC715C5DC5FE88D\" # First result is our exact SKU\n",
        "#query = \"Shades\" # Blended result\n",
        "#query = \"sunglases\" # Misspelling\n",
        "\n",
        "await run_query(ecom_db_pool, f\"SET google_columnar_engine.enable_columnar_scan={columnar_engine_state};\")\n",
        "\n",
        "sql = f\"\"\"WITH\n",
        "  -- 1. Define the pool of candidate IDs exactly once\n",
        "  candidate_ids AS (\n",
        "    WITH vector_candidates AS (\n",
        "      SELECT id, embedding <=> embedding('gemini-embedding-001', '{query}')::vector AS distance FROM products ORDER BY distance LIMIT 500\n",
        "    )\n",
        "    SELECT id FROM vector_candidates WHERE distance < 0.4\n",
        "    UNION\n",
        "    SELECT id FROM products WHERE sku = '{query}'\n",
        "    UNION\n",
        "    SELECT id FROM products WHERE fts_document @@ websearch_to_tsquery('english', '{query}')\n",
        "  ),\n",
        "  -- 2. Join products with candidates and prepare facet columns (including calculated ones)\n",
        "  products_for_faceting AS (\n",
        "    SELECT\n",
        "      p.brand,\n",
        "      p.category,\n",
        "      CASE\n",
        "        WHEN p.retail_price < 50 THEN '$0 - $49.99'\n",
        "        WHEN p.retail_price >= 50 AND p.retail_price < 100 THEN '$50 - $99.99'\n",
        "        WHEN p.retail_price >= 100 AND p.retail_price < 250 THEN '$100 - $249.99'\n",
        "        WHEN p.retail_price >= 250 AND p.retail_price < 500 THEN '$250 - $499.99'\n",
        "        WHEN p.retail_price >= 500 THEN '$500+'\n",
        "        ELSE NULL\n",
        "      END AS price_range,\n",
        "      p.retail_price\n",
        "    FROM\n",
        "      products AS p\n",
        "      JOIN candidate_ids AS c ON p.id = c.id\n",
        "  ),\n",
        "  -- 3. Calculate Aggregations using GROUPING SETS\n",
        "  facet_aggregations AS (\n",
        "    SELECT\n",
        "      COALESCE(brand, category, price_range) AS facet_value,\n",
        "      CASE\n",
        "        WHEN GROUPING(brand) = 0 THEN 'brand'\n",
        "        WHEN GROUPING(category) = 0 THEN 'category'\n",
        "        WHEN GROUPING(price_range) = 0 THEN 'price_range'\n",
        "      END AS facet_type,\n",
        "      COUNT(*) AS count,\n",
        "      MIN(retail_price) as min_price_for_ordering\n",
        "    FROM\n",
        "      products_for_faceting\n",
        "    WHERE\n",
        "        brand IS NOT NULL OR category IS NOT NULL OR price_range IS NOT NULL\n",
        "    GROUP BY\n",
        "      GROUPING SETS (\n",
        "        (brand),\n",
        "        (category),\n",
        "        (price_range)\n",
        "      )\n",
        "  )\n",
        "-- 4. Final SELECT and ORDER BY from the aggregated results\n",
        "SELECT\n",
        "  facet_value,\n",
        "  facet_type,\n",
        "  count\n",
        "FROM\n",
        "  facet_aggregations\n",
        "ORDER BY\n",
        "  facet_type ASC,\n",
        "  CASE WHEN facet_type = 'price_range' THEN min_price_for_ordering ELSE NULL END ASC NULLS LAST, -- Handle potential NULL min_price\n",
        "  count DESC,\n",
        "  facet_value ASC;\"\"\"\n",
        "\n",
        "if explain:\n",
        "  sql = 'EXPLAIN (COLUMNAR_ENGINE) ' + sql\n",
        "\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "try:\n",
        "  html_output = HTML(result.to_html(escape=False, formatters={'product_image_uri': df_image_formatter}))\n",
        "  display(html_output)\n",
        "except:\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97fdc58f",
      "metadata": {
        "id": "97fdc58f"
      },
      "source": [
        "## Use ai.if() SQL Operator for Advanced Filtering\n",
        "\n",
        "Reference: https://cloud.google.com/alloydb/docs/ai/evaluate-semantic-queries-ai-operators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8fc1a9f",
      "metadata": {
        "id": "b8fc1a9f"
      },
      "outputs": [],
      "source": [
        "# --- Set explain=True to view query performance ---\n",
        "explain = False\n",
        "\n",
        "# --- Define the primary query and the AI filter ---\n",
        "query = \"winter coat\" # Simple keyword search gives good results\n",
        "ai_filter = \"made by a luxury brand\"\n",
        "\n",
        "sql = f\"\"\"\n",
        "WITH vector_search AS (\n",
        "  SELECT embedding <=> embedding('gemini-embedding-001', '{query}')::vector AS distance,\n",
        "    name,\n",
        "    product_image_uri,\n",
        "    brand,\n",
        "    product_description,\n",
        "    category,\n",
        "    department,\n",
        "    cost,\n",
        "    retail_price,\n",
        "    sku\n",
        "  FROM products\n",
        "  ORDER BY distance\n",
        "  LIMIT 20\n",
        ") SELECT ROW_NUMBER () OVER (ORDER BY distance) AS vector_rank, *\n",
        "  FROM vector_search\n",
        "  WHERE ai.if(prompt => 'The following product {ai_filter}: ' ||\n",
        "                        ' Product name: ' || COALESCE(name, '') ||\n",
        "                        ' Brand: ' || COALESCE(brand, '') ||\n",
        "                        ' Category: ' || COALESCE(category, '') ||\n",
        "                        ' Department: ' || COALESCE(department, '') ||\n",
        "                        ' Price: ' || COALESCE(retail_price, '') ||\n",
        "                        ' Description: ' || COALESCE(product_description, ''))\n",
        "\"\"\"\n",
        "\n",
        "if explain:\n",
        "  sql = 'EXPLAIN ANALYZE ' + sql\n",
        "\n",
        "result = await run_query(ecom_db_pool, sql)\n",
        "try:\n",
        "  html_output = HTML(result.to_html(escape=False, formatters={'product_image_uri': df_image_formatter}))\n",
        "  display(html_output)\n",
        "except:\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g4-7sN9Mu3E1",
      "metadata": {
        "id": "g4-7sN9Mu3E1"
      },
      "source": [
        "## Export Prepped Data\n",
        "\n",
        "Export the data to GCS to allow for easier reproducibility and demo deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DGfOLPa1u3E6",
      "metadata": {
        "id": "DGfOLPa1u3E6"
      },
      "outputs": [],
      "source": [
        "# Reference: https://cloud.google.com/alloydb/docs/reference/rest/v1/projects.locations.clusters/export\n",
        "#            https://cloud.google.com/alloydb/docs/export-sql-file\n",
        "\n",
        "import time\n",
        "\n",
        "url = f\"https://alloydb.googleapis.com/v1/projects/{project_id}/locations/{region}/clusters/{alloydb_cluster}:export\"\n",
        "request_body = {\n",
        "   \"gcsDestination\": {\"uri\": f\"gs://{export_bucket}/alloydb_export/{alloydb_database}.sql\"},\n",
        "   \"database\": f\"{alloydb_database}\",\n",
        "   \"sqlExportOptions\": {\n",
        "      \"tables\": [\n",
        "        \"distribution_centers\",\n",
        "        \"events\",\n",
        "        \"inventory_items\",\n",
        "        \"order_items\",\n",
        "        \"orders\",\n",
        "        \"products\",\n",
        "        \"users\",\n",
        "      ]\n",
        "    }\n",
        "}\n",
        "\n",
        "result = rest_api_helper(authed_session, url, 'POST', request_body, {})\n",
        "print(f\"Kicked off export: {result}\")\n",
        "\n",
        "operation_id = result['name']\n",
        "\n",
        "operation_complete = False\n",
        "while operation_complete == False:\n",
        "  print(f\"Export still running: {operation_id}\")\n",
        "  url = f\"https://alloydb.googleapis.com/v1/{operation_id}\"\n",
        "  response = rest_api_helper(authed_session, url, 'GET', request_body, {})\n",
        "  operation_complete = response['done']\n",
        "  if operation_complete:\n",
        "    print(f\"Operation complete. Check result payload for potential errors. \\nResult: {response}\")\n",
        "    continue\n",
        "  time.sleep(5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "cymbal_shops_hybrid_search_alloydb_data_prep.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
